<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Automate and deploy a JBoss EAP cluster with Ansible</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/02/08/automate-and-deploy-jboss-eap-cluster-ansible" /><author><name>Romain Pelisse</name></author><id>edefe4d3-3bd3-407d-aaae-110b95ce03f3</id><updated>2022-02-08T07:00:00Z</updated><published>2022-02-08T07:00:00Z</published><summary type="html">&lt;p&gt;In my series introducing &lt;a href="https://developers.redhat.com/blog/2020/11/06/wildfly-server-configuration-with-ansible-collection-for-jcliff-part-1/"&gt;WildFly server configuration with Ansible collection for JCliff&lt;/a&gt;, I described how developers can use &lt;a href="https://www.ansible.com"&gt;Ansible&lt;/a&gt; to manage a standalone &lt;a href="https://developers.redhat.com/eap/download"&gt;Red Hat JBoss Enterprise Application Platform&lt;/a&gt; (JBoss EAP) instance. I've also written about using Ansible to &lt;a href="https://developers.redhat.com/articles/2021/08/30/automate-red-hat-jboss-web-server-deployments-ansible"&gt;automate Apache Tomcat and Red Hat JBoss Web Server deployments&lt;/a&gt;. In this article, we'll go a bit deeper and use Ansible to deploy a fully operational &lt;a href="http://www.mastertheboss.com/jbossas/jboss-cluster/clustering-wildfly-application-server/"&gt;cluster of JBoss EAP instances&lt;/a&gt;. I'll show you how to automate the setup of each JBoss EAP instance and how to configure the network requirements—notably, fault tolerance and high availability—using features provided by the &lt;a href="https://www.wildfly.org"&gt;WildFly&lt;/a&gt; Ansible collection.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; This article assumes that you have prior knowledge of both Ansible and basic JBoss EAP/WildFly installation. Visit the &lt;a href="https://developers.redhat.com/courses/ansible/getting-started"&gt;Ansible courses page&lt;/a&gt; to learn the fundamentals of using Ansible.&lt;/p&gt; &lt;h2&gt;Use case: Deploying a JBoss EAP cluster with Ansible&lt;/h2&gt; &lt;p&gt;For this demonstration, we want to set up and run three JBoss EAP instances in a cluster. In this context, the application servers must communicate with each other to synchronize the content of the application's session. This configuration guarantees that, if one instance fails while processing a request, another one can pick up the work without any data loss.&lt;/p&gt; &lt;p&gt;We'll use a &lt;a href="https://en.wikipedia.org/wiki/Multicast"&gt;multicast&lt;/a&gt; to discover the members of the cluster and ensure that the cluster's formation is fully automated and dynamic.&lt;/p&gt; &lt;h2&gt;Step 1: Install Ansible collection for WildFly&lt;/h2&gt; &lt;p&gt;To follow this example, you need to install the Ansible collection that provides support for JBoss EAP. Named after JBoss EAP's upstream project, &lt;a href="http://github.com/ansible-middleware/wildfly/"&gt;Ansible collection for WildFly&lt;/a&gt; is part of the &lt;code&gt;middleware_automation&lt;/code&gt; collections and supplies a set of roles to simplify automation:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ansible-galaxy collection install middleware_automation.wildfly Process install dependency map Starting collection install process Installing 'middleware_automation.wildfly:0.0.2' to '/root/.ansible/collections/ansible_collections/middleware_automation/wildfly' Installing 'middleware_automation.redhat_csp_download:1.2.1' to '/root/.ansible/collections/ansible_collections/middleware_automation/redhat_csp_download&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note the following:&lt;/p&gt; &lt;p&gt;The content of Ansible collection for WildFly comes from the &lt;a href="https://galaxy.ansible.com/wildfly/jcliff"&gt;Ansible collection for JCliff&lt;/a&gt;, which is used to help deploy applications and fine-tune server configurations. To keep things simple, we won't use JCliff's features in this example. The part of the WildFly collection that we are using has been separated from the JCliff collection so that developers can use the WildFly features without having to install JCliff. Features of JCliff are not required for all use cases involving JBoss EAP.&lt;/p&gt; &lt;p&gt;Additionally, note that the &lt;code&gt;middleware_automation&lt;/code&gt; collections are provided through &lt;a href="https://galaxy.ansible.com"&gt;Ansible Galaxy&lt;/a&gt; and are not certified for &lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;Red Hat Ansible Automation Platform&lt;/a&gt;. Those certified collections are provided by &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_ansible_automation_platform/1.2/html/getting_started_with_red_hat_ansible_automation_hub/index"&gt;Red Hat Ansible Automation Hub&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Step 2: Set up the JBoss EAP cluster&lt;/h2&gt; &lt;p&gt;A typical JBoss EAP cluster has several machines, each operating a dedicated instance. In this case, for the simplicity of testing and reproducibility on a development system, we are going to use just one machine running several instances of JBoss EAP. The WildFly collection for Ansible makes it relatively easy to set up this architecture and provides all the required plumbing.&lt;/p&gt; &lt;p&gt;There are two parts to setting up the JBoss EAP cluster:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;&lt;strong&gt;Install JBoss EAP on the hosts.&lt;/strong&gt; This installation involves authenticating against the Red Hat Network (RHN), downloading the archive, and decompressing the archive in the appropriate directory (&lt;code&gt;JBOSS_HOME&lt;/code&gt;). These tasks are handled by the &lt;code&gt;wildfly_install&lt;/code&gt; role supplied by &lt;code&gt;wildfly&lt;/code&gt; collection.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Create the configuration files to run several instances of JBoss EAP.&lt;/strong&gt; Because we're running multiple instances on a single host, you also need to ensure that each instance has its own subdirectories and set of ports, so that the instances can coexist and communicate. Fortunately, this functionality is provided by a role within the Ansible collection called &lt;code&gt;wildfly_systemd&lt;/code&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Note that if the variables &lt;code&gt;rhn_username&lt;/code&gt; and &lt;code&gt;rhn_password&lt;/code&gt; are defined, the collection automatically downloads the latest available version of JBoss EAP. If not, the &lt;code&gt;wildfly_install&lt;/code&gt; role fetches the latest upstream WildFly version. To avoid adding the credentials in our playbook, we incorporate them into a separate file named &lt;code&gt;rhn-creds.yml&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;--- rhn_username: rhn_password: jboss_eap_rhn_id: &lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; We could use &lt;a href="https://docs.ansible.com/ansible/latest/user_guide/vault.html"&gt;Ansible's vault&lt;/a&gt; feature to safely encrypt the credential values, but doing that is out of the scope of this article.&lt;/p&gt; &lt;h3&gt;Ansible playbook to install JBoss EAP&lt;/h3&gt; &lt;p&gt;Here is our Ansible playbook for installing and configuring JBoss EAP:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;--- - name: "JBoss EAP installation and configuration" hosts: "{{ hosts_group_name | default('localhost') }}" become: yes vars: wildfly_install_workdir: '/opt' wildfly_version: '7.4' install_name: jboss-eap wildfly_archive_filename: "{{ install_name }}-{{ wildfly_version }}.zip" wildfly_user: "{{ install_name }}" wildfly_config_base: standalone-ha.xml wildfly_home: "{{ wildfly_install_workdir }}/{{ install_name }}-{{ wildfly_version }}" jboss_eap_rhn_id: 99481 instance_http_ports: - 8180 - 8280 - 8380 app: name: 'info-1.1.war' url: 'https://drive.google.com/uc?export=download&amp;id=1w9ss5okctnjUvRAxhPEPyC7DmbUwmbhb' collections: - middleware_automation.redhat_csp_download - middleware_automation.wildfly roles: - redhat_csp_download - wildfly_install tasks: - name: "Set up for WildFly instance {{ item }}" include_role: name: wildfly_systemd vars: wildfly_config_base: 'standalone-ha.xml' wildfly_basedir_prefix: "/opt/{{ inventory_hostname }}" wildfly_config_name: "{{ install_name }}" wildfly_port_range_offset: -1 wildfly_instance_name: "{{ install_name }}" instance_id: "{{ item }}" service_systemd_env_file: "/etc/eap-{{ item }}.conf" service_systemd_conf_file: "/usr/lib/systemd/system/jboss-eap-{{ item }}.service" loop: "{{ range(0,3) | list }}" post_tasks: - set_fact: instance_http_ports: - 8180 - 8280 - 8380 - wait_for: port: "{{ item }}" loop: "{{ instance_http_ports }}" - name: "Checks that WildFly server is running and accessible" get_url: url: "http://localhost:{{ item }}/" dest: '/dev/null' loop: "{{ instance_http_ports }}" &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Run the playbook&lt;/h3&gt; &lt;p&gt;Now, let's run our Ansible playbook and check the resulting output:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# ansible-playbook -e @creds.yml jboss_eap.yml PLAY [WildFly installation and configuration] ****************************************************************** TASK [Gathering Facts] ***************************************************************************************** ok: [localhost] TASK [middleware_automation.redhat_csp_download.redhat_csp_download : assert] … TASK [Checks that WildFly server is running and accessible] **************************************************** ok: [localhost] =&gt; (item=8180) ok: [localhost] =&gt; (item=8280) ok: [localhost] =&gt; (item=8380) PLAY RECAP ***************************************************************************************************** localhost : ok=90 changed=0 unreachable=0 failed=0 skipped=14 rescued=0 ignored=0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Although the playbook is quite short, it performs almost 100 tasks. First, it automatically installs the dependencies for &lt;code&gt;middleware_automation.redhat_csp_download&lt;/code&gt; by adding the associated role. Then, the &lt;code&gt;wildfly_install&lt;/code&gt; role uses the provided credentials to connect to RHN and download the &lt;code&gt;jboss-eap-7.4.zip&lt;/code&gt; file. Finally, once those steps have been completed successfully, the &lt;code&gt;wildfly_systemd&lt;/code&gt; role sets up three distinct services, each with its own set of ports and directory layout to store instance-specific data.&lt;/p&gt; &lt;p&gt;Note that the JBoss EAP installation is &lt;strong&gt;not duplicated&lt;/strong&gt;. All of the binaries live under the &lt;code&gt;/opt/jboss-eap-74&lt;/code&gt; directory. The separate directories simply store the runtime data for each instance.&lt;/p&gt; &lt;p&gt;On top of everything, we configured the instances to use the &lt;code&gt;standalone-ha.xml&lt;/code&gt; configuration as the baseline, so they are already set up for clustering.&lt;/p&gt; &lt;h2&gt;Step 3: Confirm the JBoss EAP instance and services are running&lt;/h2&gt; &lt;p&gt;The playbook confirms that each instance can be reached through its own HTTP port. We can also verify that the services are running by using the &lt;code&gt;systemctl&lt;/code&gt; command:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# systemctl status jboss-eap-* ● jboss-eap-0.service - JBoss EAP (standalone mode) Loaded: loaded (/usr/lib/systemd/system/jboss-eap-0.service; enabled; vendor preset: disabled) Active: active (running) since Thu 2021-12-23 12:31:41 UTC; 1min 55s ago Main PID: 1138 (standalone.sh) Tasks: 70 (limit: 1638) Memory: 532.3M CGroup: /system.slice/jboss-eap-0.service ├─1138 /bin/sh /opt/jboss-eap-7.4/bin/standalone.sh -c jboss-eap-0.xml -b 0.0.0.0 -Djboss.server.con&gt; └─1261 java -D[Standalone] -server -verbose:gc -Xloggc:/opt/localhost0/log/gc.log -XX:+PrintGCDetail&gt; Dec 23 12:31:44 7b38800644ee standalone.sh[1138]: 12:31:44,548 INFO [org.jboss.as.patching] (MSC service thread) Dec 23 12:31:44 7b38800644ee standalone.sh[1138]: 12:31:44,563 WARN [org.jboss.as.domain.management.security] &gt; Dec 23 12:31:44 7b38800644ee standalone.sh[1138]: 12:31 …&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Step 4: Deploy an application to the JBoss EAP cluster&lt;/h2&gt; &lt;p&gt;At this point, the three JBoss EAP instances are configured for clustering. However, no applications were deployed, so the cluster is not active (there is nothing to keep synchronized between all the instances).&lt;/p&gt; &lt;p&gt;Let's modify our Ansible playbook to deploy a simple application to all three JBoss EAP instances. To achieve this, we'll leverage another role provided by the &lt;code&gt;wildfly&lt;/code&gt; collection: &lt;code&gt;jboss_eap&lt;/code&gt;. This role includes a set of tasks generally focused on features specific to JBoss EAP.&lt;/p&gt; &lt;p&gt;In our case, we will use the &lt;code&gt;jboss_cli.yml&lt;/code&gt; task file, which encapsulates the running of JBoss command-line interface (CLI) queries:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;… - name: "Ensures webapp {{ app.name }} has been retrieved from {{ app.url }}" get_url: url: "{{ app.url }}" dest: "{{ wildfly_install_workdir }}/{{ app.name }}" - name: "Deploy webapp" include_role: name: jboss_eap tasks_from: jboss_cli.yml vars: jboss_home: "{{ wildfly_home }}" query: "'deploy --force {{ wildfly_install_workdir }}/{{ app.name }}'" jboss_cli_controller_port: "{{ item }}" loop: - 10090 - 10190 - 10290 …&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now, we will once again execute our playbook so that the web application is deployed on all instances. Once the automation completes successfully, the deployment will trigger the formation of the cluster.&lt;/p&gt; &lt;h2&gt;Step 5: Verify the JBoss EAP cluster and application deployment&lt;/h2&gt; &lt;p&gt;You can verify the JBoss EAP cluster formation by looking at the log files of any of the three JBoss EAP instances:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;… 021-12-23 15:02:08,252 INFO [org.infinispan.CLUSTER] (thread-7,ejb,jboss-eap-0) ISPN000094: Received new cluster view for channel ejb: [jboss-eap-0|2] (3) [jboss-eap-0, jboss-eap-1, jboss-eap-2] …&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To be thorough, you can also check that the application is properly deployed and accessible. To validate the application's operation, we can simply add a separate Ansible playbook called &lt;code&gt;validate.yml&lt;/code&gt;. We can then import the new playbook into our &lt;code&gt;playbook.yml&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;post_tasks: - include_tasks: validate.yml loop: "{{ instance_http_ports }}"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;validate.yml&lt;/code&gt; file contains the following:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;--- - assert: that: - item is defined - wait_for: port: "{{ item }}" - name: "Checks that WildFly server is running and accessible on port {{ item }}" get_url: url: "http://localhost:{{ item }}/" dest: '/dev/null' changed_when: False - include_tasks: info.yml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You might have noticed that we include another playbook, &lt;code&gt;info.yml&lt;/code&gt;, which is here:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;--- - assert: that: - item is defined quiet: true - set_fact: result_file: "/tmp/info-{{ item }}.txt" - get_url: url: "http://localhost:{{ item }}/info/" dest: "{{ result_file }}" changed_when: False - slurp: src: "{{ result_file }}" register: info_res - debug: msg: "{{ info_res['content'] | b64decode }}&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To complete the exercise, we can run the validation playbook and see whether it confirms that our setup is fully functional:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;TASK [get_url] ******************************************************************************** changed: [localhost] TASK [slurp] ********************************************************************************** ok: [localhost] TASK [debug] ********************************************************************************** ok: [localhost] =&gt; { "msg": "Request received&lt;br/&gt;Requested URL:\t\t\thttp://localhost:8380/info/&lt;br/&gt;Runs on node:\t\t\tda953ac17443 [IP: 10.0.2.100 ]&lt;br/&gt;Requested by:\t\t\t127.0.0.1 [IP: 127.0.0.1, port: 40334 ]&lt;br/&gt;JBOSS_ID:\t\t\tnull&lt;br/&gt;" }&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Next steps with Ansible and JBoss EAP&lt;/h2&gt; &lt;p&gt;In this article, we've fully automated the setup and configuration of a three-instance cluster of JBoss EAP, along with an example web application as a workload. The playbook that we created for the automation is simple and straightforward. Most importantly, we were able to focus primarily on deploying the application. The WildFly collection for Ansible provided all the plumbing we needed to set up the JBoss EAP cluster.&lt;/p&gt; &lt;p&gt;You can find the source code for the example in the &lt;a href="https://github.com/ansible-middleware/wildfly-cluster-demo"&gt;WildFly cluster demo&lt;/a&gt; GitHub repository. For a more complex scenario, see the &lt;a href="https://github.com/ansible-middleware/flange-demo"&gt;Flange project demo&lt;/a&gt;, which adds to the JBoss EAP cluster an instance of Red Hat's &lt;a href="https://access.redhat.com/products/red-hat-single-sign-on"&gt;single sign-on technology&lt;/a&gt;, using &lt;a href="https://developers.redhat.com/products/datagrid/overview"&gt;Red Hat Data Grid&lt;/a&gt; as a cache and &lt;a href="https://www.redhat.com/en/resources/jboss-core-services-collection-datasheet"&gt;Red Hat Middleware Core Services Collection&lt;/a&gt; as a load balancer.&lt;/p&gt; &lt;p&gt;See these resources to learn more about using Ansible with JBoss EAP:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/08/30/automate-red-hat-jboss-web-server-deployments-ansible"&gt;Automate Red Hat JBoss Web Server deployments with Ansible&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/09/28/set-modcluster-red-hat-jboss-web-server-ansible"&gt;Set up mod_cluster for Red Hat JBoss Web Server with Ansible&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/blog/2020/11/06/wildfly-server-configuration-with-ansible-collection-for-jcliff-part-1"&gt;WildFly server configuration with Ansible collection for JCliff (three-part series)&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;For more hands-on learning, in short, interactive courses, see the &lt;a href="https://developers.redhat.com/courses/ansible"&gt;Ansible courses page&lt;/a&gt;. Also, be sure to check out &lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;Red Hat Ansible Automation Platform&lt;/a&gt; for provisioning, deploying, and managing IT infrastructure across cloud, virtual, and physical environments.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/02/08/automate-and-deploy-jboss-eap-cluster-ansible" title="Automate and deploy a JBoss EAP cluster with Ansible"&gt;Automate and deploy a JBoss EAP cluster with Ansible&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Romain Pelisse</dc:creator><dc:date>2022-02-08T07:00:00Z</dc:date></entry><entry><title>Investigating the cost of Open vSwitch upcalls in Linux</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/02/07/investigating-cost-open-vswitch-upcalls-linux" /><author><name>Eelco Chaudron</name></author><id>bc412619-1b36-4bc8-88ac-f33efd3af3ba</id><updated>2022-02-07T07:00:00Z</updated><published>2022-02-07T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://www.openvswitch.org/"&gt;Open vSwitch (OVS)&lt;/a&gt;, which many data centers run on &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt; systems for advanced networking functions, adds a certain amount of overhead for new datapath flows. This article introduces the &lt;code&gt;upcall_cost.py&lt;/code&gt; script, written in &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt;, which tracks packets through the kernel's invocation of OVS and displays statistics that can help with troubleshooting performance in applications and data centers, as well as the kernel and OVS itself.&lt;/p&gt; &lt;p&gt;My interest in this question was triggered when some people argued that it takes too long for the kernel module to bring a packet to the &lt;code&gt;ovs-vswitchd&lt;/code&gt; in userspace to do the lookup. However, I have not seen anyone backing up this complaint with data. This article offers tools that can help research this question and many others. I'll describe the script and its output, then show data from two interesting scenarios: A 16-node &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; cluster and a lab benchmark.&lt;/p&gt; &lt;h2 id="measuring-the-cost"&gt;The upcall_cost.py script&lt;/h2&gt; &lt;p&gt;Up till now, there was no easy way to measure the overhead introduced by the OVS kernel module, whose tasks include sending packets to the &lt;code&gt;ovs-vswitchd&lt;/code&gt; user space daemon for the OpenFlow lookup, programming the kernel datapath flow, and re-inserting the packets for forwarding. The &lt;code&gt;upcall_cost.py&lt;/code&gt; script was introduced as part of an OVS &lt;a href="https://patchwork.ozlabs.org/project/openvswitch/list/?series=276751&amp;state=*"&gt;patch series&lt;/a&gt; and demoed as part of the 2021 OVS conference presentation titled &lt;a href="http://www.openvswitch.org/support/ovscon2021/#T1"&gt;Debugging OVS using static tracepoints&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;But just having a script to get these statistics does not make capturing and analyzing this data straightforward. Every environment and use case is different and hence might give different results. Later on, we will discuss some data based on a 16-cluster &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; setup emulating a customer environment.&lt;/p&gt; &lt;h3 id="what-will-upcall_costpy-measure"&gt;Running the upcall_cost.py script&lt;/h3&gt; &lt;p&gt;I've devoted a section of this article to explaining how to run the script, because several of the options are important.&lt;/p&gt; &lt;p&gt;Make sure that, along with downloading the script, you have installed all the required Python modules. See the script's header for the requirements.&lt;/p&gt; &lt;p&gt;The script queries the configured OVS ports only at startup. So if you have a dynamic environment, you might need to modify the script or make sure all ports are configured before starting the script.&lt;/p&gt; &lt;p&gt;Executing the script with the &lt;code&gt;--help&lt;/code&gt; option shows all the available options:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;# ./upcall_cost.py --help usage: upcall_cost.py [-h] [-b BUCKETS] [--buffer-page-count NUMBER] [-D [DEBUG]] [-f [64-2048]] [--handler-filter HANDLERS] [-P [64-2048]] [-p VSWITCHD_PID] [-q] [-r FILE] [--sets] [-s EVENTS] [-w FILE] optional arguments: -h, --help show this help message and exit -b BUCKETS, --histogram-buckets BUCKETS Number of buckets per histogram, default 20 --buffer-page-count NUMBER Number of BPF ring buffer pages, default 1024 -D [DEBUG], --debug [DEBUG] Enable eBPF debugging -f [64-2048], --flow-key-size [64-2048] Set maximum flow key size to capture, default 64 --handler-filter HANDLERS Post processing handler thread filter -P [64-2048], --packet-size [64-2048] Set maximum packet size to capture, default 256 -p VSWITCHD_PID, --pid VSWITCHD_PID ovs-vswitch's PID -q, --quiet Do not show individual events -r FILE, --read-events FILE Read events from FILE instead of installing tracepoints --sets Dump content of data sets -s EVENTS, --stop EVENTS Stop after receiving EVENTS number of trace events -w FILE, --write-events FILE Write events to FILE&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Although the defaults are fine for most options, a few are worth discussing here.&lt;/p&gt; &lt;p&gt;First, I recommend running the script with the &lt;code&gt;--write-events&lt;/code&gt; or &lt;code&gt;-w&lt;/code&gt; option to save a copy of the trace information. This output can be used later to analyze the results.&lt;/p&gt; &lt;p&gt;Initially, you might want to see all the output, but it could be voluminous and introduce a delay. Therefore, after you have verified that events are coming in, I suggest running the command with the &lt;code&gt;--quiet&lt;/code&gt; or &lt;code&gt;-q&lt;/code&gt; option.&lt;/p&gt; &lt;p&gt;To make sure you don't miss any trace events, your &lt;a href="https://www.kernel.org/doc/html/latest/bpf/ringbuf.html"&gt;BPF ring buffer&lt;/a&gt; needs to be configured to be large enough to keep the events. The following example shows that the default configured size of 1,024 pages is not big enough. This problem is indicated by the &lt;code&gt;WARNING&lt;/code&gt; message as well as the counts in square brackets further down in the output showing how many events were missed:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;# ./upcall_cost.py -q - Compiling eBPF programs... - Capturing events [Press ^C to stop]... ^C - Analyzing results (35486 events)... WARNING: Not all events were captured! Increase the BPF ring buffer size with the --buffer-page-count option. =&gt; Events received per type (usable/total) [missed events]: dpif_netlink_operate__op_flow_execute : 4009/ 4009 [ 45988] dpif_netlink_operate__op_flow_put : 16/ 16 [ 112] dpif_recv__recv_upcall : 7068/ 7068 [ 42928] ktrace__ovs_packet_cmd_execute : 7581/ 7581 [ 42416] openvswitch__dp_upcall : 16812/ 16812 [ 189917] ...&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you are losing events, increase the number of pages using the &lt;code&gt;--buffer-page-count&lt;/code&gt; option until you no longer see the &lt;code&gt;WARNING&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;# ./upcall_cost.py -q --buffer-page-count 16385 - Compiling eBPF programs... - Capturing events [Press ^C to stop]... ^C - Analyzing results (151493 events)... =&gt; Events received per type (usable/total) [missed events]: dpif_netlink_operate__op_flow_execute : 31618/ 31618 [ 0] dpif_netlink_operate__op_flow_put : 256/ 256 [ 0] dpif_recv__recv_upcall : 31618/ 31618 [ 0] ktrace__ovs_packet_cmd_execute : 31618/ 31618 [ 0] openvswitch__dp_upcall : 56383/ 56383 [ 0] ...&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;What does the upcall_cost.py output mean?&lt;/h3&gt; &lt;p&gt;To show &lt;code&gt;upcall_cost.py&lt;/code&gt; in action, this section runs the script when 10 packets arrive at wire speed with incrementing source and destination IPv4 addresses. These packets will create 10 upcalls, where each will install a new datapath flow. The following example omits the &lt;code&gt;--quiet&lt;/code&gt; option.&lt;/p&gt; &lt;p&gt;The initial output is:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ./upcall_cost.py - Compiling eBPF programs... - Capturing events [Press ^C to stop]... EVENT COMM PID CPU TIME EVENT DATA[dpif_name/dp_port/pkt_len/pkt_frag_len] [openvswitch__dp_upcall] swapper/8 0 [008] 1802346.842674263: ovs-system 2 60 0 [openvswitch__dp_upcall] swapper/24 0 [024] 1802346.842674671: ovs-system 2 60 0 [openvswitch__dp_upcall] swapper/22 0 [022] 1802346.842674792: ovs-system 2 60 0 [openvswitch__dp_upcall] swapper/13 0 [013] 1802346.842674989: ovs-system 2 60 0 [openvswitch__dp_upcall] swapper/0 0 [000] 1802346.842675215: ovs-system 2 60 0 [openvswitch__dp_upcall] swapper/8 0 [008] 1802346.842685418: ovs-system 2 60 0 [openvswitch__dp_upcall] swapper/22 0 [022] 1802346.842686394: ovs-system 2 60 0 [openvswitch__dp_upcall] swapper/8 0 [008] 1802346.842688965: ovs-system 2 60 0 [openvswitch__dp_upcall] swapper/8 0 [008] 1802346.842692699: ovs-system 2 60 0 [dpif_recv__recv_upcall] handler10 409283 [024] 1802346.842702062: ovs-system 2 60 [dpif_recv__recv_upcall] handler8 409281 [009] 1802346.842702155: ovs-system 2 60 [dpif_recv__recv_upcall] handler15 409288 [014] 1802346.842703571: ovs-system 2 60 [dpif_recv__recv_upcall] handler1 409274 [022] 1802346.842710813: ovs-system 2 60 [dpif_recv__recv_upcall] handler8 409281 [009] 1802346.842732671: ovs-system 2 60 [dpif_recv__recv_upcall] handler10 409283 [024] 1802346.842734368: ovs-system 2 60 [..operate__op_flow_put] handler1 409274 [022] 1802346.842736406 [..ate__op_flow_execute] handler1 409274 [022] 1802346.842737759: 60 [dpif_recv__recv_upcall] handler10 409283 [024] 1802346.842741405: ovs-system 2 60 [..operate__op_flow_put] handler8 409281 [009] 1802346.842741889 [..ate__op_flow_execute] handler8 409281 [009] 1802346.842743288: 60 [..operate__op_flow_put] handler8 409281 [009] 1802346.842744921 [..operate__op_flow_put] handler15 409288 [014] 1802346.842745244 [..ate__op_flow_execute] handler8 409281 [009] 1802346.842745913: 60 [..ate__op_flow_execute] handler15 409288 [014] 1802346.842746802: 60 [dpif_recv__recv_upcall] handler10 409283 [024] 1802346.842747036: ovs-system 2 60 [dpif_recv__recv_upcall] handler10 409283 [024] 1802346.842752903: ovs-system 2 60 [..s_packet_cmd_execute] handler1 409274 [022] 1802346.842757603 [..s_packet_cmd_execute] handler15 409288 [014] 1802346.842758324 [..s_packet_cmd_execute] handler8 409281 [009] 1802346.842759437 [..operate__op_flow_put] handler10 409283 [024] 1802346.842761813 [..ate__op_flow_execute] handler10 409283 [024] 1802346.842763171: 60 [..operate__op_flow_put] handler10 409283 [024] 1802346.842764504 [..s_packet_cmd_execute] handler8 409281 [009] 1802346.842765386 [..ate__op_flow_execute] handler10 409283 [024] 1802346.842765481: 60 [..operate__op_flow_put] handler10 409283 [024] 1802346.842766751 [..ate__op_flow_execute] handler10 409283 [024] 1802346.842767685: 60 [..operate__op_flow_put] handler10 409283 [024] 1802346.842768766 [..ate__op_flow_execute] handler10 409283 [024] 1802346.842769672: 60 [..operate__op_flow_put] handler10 409283 [024] 1802346.842770857 [..ate__op_flow_execute] handler10 409283 [024] 1802346.842771819: 60 [..s_packet_cmd_execute] handler10 409283 [024] 1802346.842784813 [..s_packet_cmd_execute] handler10 409283 [024] 1802346.842790567 [..s_packet_cmd_execute] handler10 409283 [024] 1802346.842793300 [..s_packet_cmd_execute] handler10 409283 [024] 1802346.842795959 [..s_packet_cmd_execute] handler10 409283 [024] 1802346.842798544 [openvswitch__dp_upcall] swapper/25 0 [025] 1802346.862490199: ovs-system 2 60 0 [dpif_recv__recv_upcall] handler11 409284 [011] 1802346.862515346: ovs-system 2 60 [..operate__op_flow_put] handler11 409284 [011] 1802346.862533071 [..ate__op_flow_execute] handler11 409284 [011] 1802346.862534409: 60 [..s_packet_cmd_execute] handler11 409284 [011] 1802346.862546508 ^C&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, the output shows the total number of events received and their types. This kind of output also appeared in examples in the previous section because it indicates when there are potentially missed events:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;- Analyzing results (50 events)... =&gt; Events received per type (usable/total) [missed events]: dpif_netlink_operate__op_flow_execute : 10/ 10 [ 0] dpif_netlink_operate__op_flow_put : 10/ 10 [ 0] dpif_recv__recv_upcall : 10/ 10 [ 0] ktrace__ovs_packet_cmd_execute : 10/ 10 [ 0] openvswitch__dp_upcall : 10/ 10 [ 0]&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, the script starts correlating events. The following example shows 10 sets of correlated events, which is as expected, because we have sent 10 packets without a datapath flow, so they needed full processing:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;- Matching DP_UPCALLs to RECV_UPCALLs |████████████████████████████████████████| 10/10 [100%] in 0.0s (478.15/s) - Analyzing 10 event sets...&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next are statistics related to which handler threads processed the upcall by calling &lt;code&gt;dpif_recv()&lt;/code&gt; and reading the data from the socket. These statistics might be interesting to see whether the upcalls are distributed evenly across threads. If not, this might be an interesting area for research. This imbalance could be perfectly fine because you might only have a limited set of flows, but it could also be the sign of a bug or misconfiguration:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;=&gt; Upcalls handled per thread: handler10 : 5 handler8 : 2 handler15 : 1 handler1 : 1 handler11 : 1&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Batching and event types&lt;/h3&gt; &lt;p&gt;Before looking at the next set of histograms, we need to talk about the events being captured and their relationships. We'll look at five types of events, two executed in the kernel and three by OVS through Userland Statically Defined Tracing (USDT) probes.&lt;/p&gt; &lt;p&gt;The events generated within the kernel are:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;openvswitch__dp_upcall&lt;/code&gt;: A kernel tracepoint, which appears every time the OVS kernel module wants to send a packet to userspace.&lt;/li&gt; &lt;li&gt;&lt;code&gt;ktrace__ovs_packet_cmd_execute&lt;/code&gt;: A &lt;code&gt;kprobe&lt;/code&gt;, which appears every time the OVS kernel module receives a packet from userspace that it needs to execute, i.e. forward.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The events generated by USDT probes in OVS are:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;dpif_recv__recv_upcall&lt;/code&gt;: An event that appears every time an upcall is read from the kernel.&lt;/li&gt; &lt;li&gt;&lt;code&gt;dpif_netlink_operate__op_flow_put&lt;/code&gt;: An event that appears every time the Netlink datapath is about to execute the &lt;code&gt;DPIF_OP_FLOW_PUT&lt;/code&gt; operation as part of the &lt;code&gt;dpif_operate()&lt;/code&gt; callback.&lt;/li&gt; &lt;li&gt;&lt;code&gt;dpif_netlink_operate__op_flow_execute&lt;/code&gt;: An event that appears every time the Netlink datapath is about to execute the &lt;code&gt;DPIF_OP_FLOW_EXECUTE&lt;/code&gt; operation as part of the &lt;code&gt;dpif_operate()&lt;/code&gt; callback.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;OVS usually reads upcall messages in the &lt;code&gt;openvswitch__dp_upcall&lt;/code&gt; function in batches, with a maximum of 64 messages per batch. When OVS doesn't batch the upcalls, the sequence of events is:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;openvswitch__dp_upcall&lt;/code&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;dpif_recv__recv_upcall&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;dpif_netlink_operate__op_flow_put&lt;/code&gt; &lt;sup&gt;1&lt;/sup&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;dpof_netlink_operate__op_flow_execute&lt;/code&gt; &lt;sup&gt;2&lt;/sup&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;ktrace__ovs_packet_cmd_execute&lt;/code&gt; &lt;sup&gt;2&lt;/sup&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Note that the flow put operation, &lt;code&gt;dpif_netlink_operate__op_flow_put&lt;/code&gt;, might be missing if the datapath flow was already pushed to the kernel. This can happen when two or more packets of the same flow are queued for slow-path processing.&lt;/p&gt; &lt;p&gt;Also note that the flow execution operations, &lt;code&gt;dpof_netlink_operate__op_flow_execute&lt;/code&gt; and &lt;code&gt;ace__ovs_packet_cmd_execute&lt;/code&gt;,  might be missing if, for example, the packet was sent up for forwarding to the controller.&lt;/p&gt; &lt;p&gt;More often, OVS batches the received upcalls. To clarify the sequence in this case, we take the example of three upcalls called A, B, and C:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;openvswitch__dp_upcall&lt;/code&gt; &lt;strong&gt;A&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;openvswitch__dp_upcall&lt;/code&gt; &lt;strong&gt;B&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;openvswitch__dp_upcall&lt;/code&gt; &lt;strong&gt;C&lt;/strong&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;dpif_recv__recv_upcall&lt;/code&gt; &lt;strong&gt;A&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;dpif_recv__recv_upcall&lt;/code&gt; &lt;strong&gt;B&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;dpif_recv__recv_upcall&lt;/code&gt; &lt;strong&gt;C&lt;/strong&gt; &lt;ul&gt;&lt;li&gt;&lt;em&gt;OVS is doing all the flow lookups/preprocessing for A, B, and C.&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;dpif_netlink_operate__op_flow_put&lt;/code&gt; &lt;strong&gt;A&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;dpif_netlink_operate__op_flow_execute&lt;/code&gt; &lt;strong&gt;A&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;dpif_netlink_operate__op_flow_put&lt;/code&gt; &lt;strong&gt;B&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;dpif_netlink_operate__op_flow_execute&lt;/code&gt; &lt;strong&gt;B&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;dpif_netlink_operate__op_flow_put&lt;/code&gt; &lt;strong&gt;C&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;dpif_netlink_operate__op_flow_execute&lt;/code&gt; &lt;strong&gt;C&lt;/strong&gt; &lt;ul&gt;&lt;li&gt;&lt;em&gt;OVS sends the batched netlink message to the kernel.&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;ktrace__ovs_packet_cmd_execute&lt;/code&gt; &lt;strong&gt;A&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;ktrace__ovs_packet_cmd_execute&lt;/code&gt; &lt;strong&gt;B&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;ktrace__ovs_packet_cmd_execute&lt;/code&gt; &lt;strong&gt;C&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Histograms&lt;/h3&gt; &lt;p&gt;Now that we know the sequence of events, let's discuss the output of the first histogram. This one is straightforward: It shows the total number of sets (the total number of sequences for a single packet, as explained earlier), the minimum batch size, and the maximum batch size:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;=&gt; Histogram of upcalls per batch: # NumSamples = 5; Min = 1; Max = 5 # each ∎ represents a count of 1 1 [ 3]: ∎∎∎ 33 [ 0]: 2 [ 1]: ∎ 34 [ 0]: 3 [ 0]: 35 [ 0]: 4 [ 0]: 36 [ 0]: 5 [ 1]: ∎ 37 [ 0]: 6 [ 0]: 38 [ 0]: 7 [ 0]: 39 [ 0]: 8 [ 0]: 40 [ 0]: 9 [ 0]: 41 [ 0]: 10 [ 0]: 42 [ 0]: 11 [ 0]: 43 [ 0]: 12 [ 0]: 44 [ 0]: 13 [ 0]: 45 [ 0]: 14 [ 0]: 46 [ 0]: 15 [ 0]: 47 [ 0]: 16 [ 0]: 48 [ 0]: 17 [ 0]: 49 [ 0]: 18 [ 0]: 50 [ 0]: 19 [ 0]: 51 [ 0]: 20 [ 0]: 52 [ 0]: 21 [ 0]: 53 [ 0]: 22 [ 0]: 54 [ 0]: 23 [ 0]: 55 [ 0]: 24 [ 0]: 56 [ 0]: 25 [ 0]: 57 [ 0]: 26 [ 0]: 58 [ 0]: 27 [ 0]: 59 [ 0]: 28 [ 0]: 60 [ 0]: 29 [ 0]: 61 [ 0]: 30 [ 0]: 62 [ 0]: 31 [ 0]: 63 [ 0]: 32 [ 0]: 64 [ 0]:&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The next four histograms have a slightly different format, but still show the number of samples (sets) and the minimum and maximum value as before. In addition, the output shows some statistical properties: mean, variance, standard deviation (SD), and median. All times are in microseconds.&lt;/p&gt; &lt;p&gt;The first histogram we'll examine shows the delta between the &lt;code&gt;openvswitch__dp_upcall&lt;/code&gt; and &lt;code&gt;dpif_recv__recv_upcall&lt;/code&gt; events: In other words, the time it takes from the kernel queueing the upcall data till the &lt;code&gt;ovs-vswitchd&lt;/code&gt; reads it from the Netlink socket:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;=&gt; Kernel upcall action to vswitchd receive (microseconds): # NumSamples = 10; Min = 25.15; Max = 60.20 # Mean = 42.472500; Variance = 206.186660; SD = 14.359201; Median 40.937500 # each ∎ represents a count of 1 25.1470 - 26.8998 [ 1]: ∎ 26.8998 - 28.6527 [ 3]: ∎∎∎ 28.6527 - 30.4055 [ 0]: 30.4055 - 32.1584 [ 0]: 32.1584 - 33.9112 [ 0]: 33.9112 - 35.6641 [ 1]: ∎ 35.6641 - 37.4169 [ 0]: 37.4169 - 39.1698 [ 0]: 39.1698 - 40.9226 [ 0]: 40.9226 - 42.6755 [ 0]: 42.6755 - 44.4283 [ 0]: 44.4283 - 46.1812 [ 0]: 46.1812 - 47.9340 [ 1]: ∎ 47.9340 - 49.6869 [ 0]: 49.6869 - 51.4398 [ 0]: 51.4398 - 53.1926 [ 0]: 53.1926 - 54.9454 [ 0]: 54.9454 - 56.6983 [ 1]: ∎ 56.6983 - 58.4511 [ 1]: ∎ 58.4511 - 60.2040 [ 2]: ∎∎&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following histogram shows the reverse of the previous one, meaning the delta between the &lt;code&gt;dpif_netlink_operate__op_flow_execute&lt;/code&gt; and &lt;code&gt;ktrace__ovs_packet_cmd_execute&lt;/code&gt; events: In other words, the time it takes from the &lt;code&gt;ovs-vswitchd&lt;/code&gt; queuing the &lt;code&gt;DPIF_OP_FLOW_EXECUTE&lt;/code&gt; operation till the OVS kernel module receives it:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;=&gt; vswitchd execute to kernel receive (microseconds): # NumSamples = 10; Min = 11.52; Max = 26.73 # Mean = 20.444200; Variance = 29.230519; SD = 5.406526; Median 20.743000 # each ∎ represents a count of 1 11.5220 - 12.2821 [ 2]: ∎∎ 12.2821 - 13.0423 [ 0]: 13.0423 - 13.8025 [ 0]: 13.8025 - 14.5626 [ 0]: 14.5626 - 15.3228 [ 0]: 15.3228 - 16.0829 [ 0]: 16.0829 - 16.8431 [ 1]: ∎ 16.8431 - 17.6032 [ 0]: 17.6032 - 18.3634 [ 0]: 18.3634 - 19.1235 [ 0]: 19.1235 - 19.8837 [ 2]: ∎∎ 19.8837 - 20.6438 [ 0]: 20.6438 - 21.4040 [ 0]: 21.4040 - 22.1641 [ 1]: ∎ 22.1641 - 22.9243 [ 0]: 22.9243 - 23.6844 [ 0]: 23.6844 - 24.4445 [ 0]: 24.4445 - 25.2047 [ 1]: ∎ 25.2047 - 25.9649 [ 1]: ∎ 25.9649 - 26.7250 [ 2]: ∎∎&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following histogram shows the total time it takes from the &lt;code&gt;openvswitch__dp_upcall&lt;/code&gt; event till the &lt;code&gt;ktrace__ovs_packet_cmd_execute&lt;/code&gt; event, minus the time &lt;code&gt;ovs-vswitchd&lt;/code&gt; spends doing the actual flow lookup/preparation. Put mathematically, the value reflects:&lt;/p&gt; &lt;p&gt;ΔT=(&lt;code&gt;ktrace__ovs_packet_cmd_execute&lt;/code&gt; − &lt;code&gt;openvswitch__dp_upcall&lt;/code&gt;) - (&lt;code&gt;dpif_netlink_operate__op_flow_put&lt;/code&gt; − &lt;code&gt;dpif_recv__recv_upcall&lt;/code&gt;)&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;=&gt; Upcall overhead (total time minus lookup) (microseconds): # NumSamples = 10; Min = 38.58; Max = 87.89 # Mean = 64.094400; Variance = 361.183667; SD = 19.004833; Median 61.768500 # each ∎ represents a count of 1 38.5840 - 41.0494 [ 1]: ∎ 41.0494 - 43.5147 [ 1]: ∎ 43.5147 - 45.9801 [ 1]: ∎ 45.9801 - 48.4454 [ 0]: 48.4454 - 50.9108 [ 1]: ∎ 50.9108 - 53.3761 [ 0]: 53.3761 - 55.8415 [ 0]: 55.8415 - 58.3068 [ 1]: ∎ 58.3068 - 60.7722 [ 0]: 60.7722 - 63.2375 [ 0]: 63.2375 - 65.7028 [ 0]: 65.7028 - 68.1682 [ 1]: ∎ 68.1682 - 70.6335 [ 0]: 70.6335 - 73.0989 [ 0]: 73.0989 - 75.5643 [ 0]: 75.5643 - 78.0296 [ 0]: 78.0296 - 80.4950 [ 0]: 80.4950 - 82.9603 [ 1]: ∎ 82.9603 - 85.4257 [ 1]: ∎ 85.4257 - 87.8910 [ 2]: ∎∎&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The final histogram shows the total time it takes from the &lt;code&gt;openvswitch__dp_upcall&lt;/code&gt; event till the &lt;code&gt;ktrace__ovs_packet_cmd_execute&lt;/code&gt; event:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;=&gt; Kernel upcall to kernel packet execute (microseconds): # NumSamples = 10; Min = 56.31; Max = 116.30 # Mean = 93.283600; Variance = 323.670661; SD = 17.990849; Median 95.245000 # each ∎ represents a count of 1 56.3090 - 59.3087 [ 1]: ∎ 59.3087 - 62.3085 [ 0]: 62.3085 - 65.3083 [ 0]: 65.3083 - 68.3080 [ 0]: 68.3080 - 71.3077 [ 0]: 71.3077 - 74.3075 [ 0]: 74.3075 - 77.3072 [ 0]: 77.3072 - 80.3070 [ 1]: ∎ 80.3070 - 83.3067 [ 1]: ∎ 83.3067 - 86.3065 [ 2]: ∎∎ 86.3065 - 89.3063 [ 0]: 89.3063 - 92.3060 [ 0]: 92.3060 - 95.3057 [ 0]: 95.3057 - 98.3055 [ 0]: 98.3055 - 101.3053 [ 0]: 101.3053 - 104.3050 [ 0]: 104.3050 - 107.3047 [ 2]: ∎∎ 107.3047 - 110.3045 [ 2]: ∎∎ 110.3045 - 113.3042 [ 0]: 113.3042 - 116.3040 [ 1]: ∎&lt;/code&gt;&lt;/pre&gt; &lt;h2 id="getting-some-real-world-data"&gt;Runs with real-world data&lt;/h2&gt; &lt;p&gt;The data we will present next was taken on a node within a 16-node OpenShift cluster running &lt;a href="https://www.ovn.org/"&gt;Open Virtual Network (OVN)&lt;/a&gt; on top of OVS, over the duration of two hours and 20 minutes. The goal was to simulate a real-world traffic set, using both TCP and UDP with a mix of streamed and request-response-like traffic.&lt;/p&gt; &lt;p&gt;Before we started the script, we made sure all pods needed in the cluster were up and running, so no OVS port modifications were required. (Some modifications did happen during the run, but those events were ignored).&lt;/p&gt; &lt;p&gt;Let's dump the output of the run and add some remarks to it:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# ./upcall_cost.py -q -r ovn_ocp_stats.dat - Reading events from "ovn_ocp_stats.dat"... - Analyzing results (309351 events)... =&gt; Events received per type (usable/total) [missed events]: dpif_netlink_operate__op_flow_execute : 65294/ 65294 [ 0] dpif_netlink_operate__op_flow_put : 56135/ 56135 [ 0] dpif_recv__recv_upcall : 64969/ 64969 [ 0] ktrace__ovs_packet_cmd_execute : 65292/ 65292 [ 0] openvswitch__dp_upcall : 57661/ 64224 [ 0] - Matching DP_UPCALLs to RECV_UPCALLs |████████████████████████████████████████| 57661/57661 [100%] in 10.1s (5683.50/s) WARNING: SKB from kernel had fragments, we could only copy/compare the first part! - Analyzing 57538 event sets...&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The first thing to notice is that the upcalls are not nicely distributed among the handler threads. There could be all kinds of reasons, which might be worth investigating. Because the saved events have a lot of data, including a packet fragment, it's possible to write an additional script the analyze the data:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;=&gt; Upcalls handled per thread: handler1 : 26769 handler2 : 21067 handler5 : 5148 handler6 : 2665 handler4 : 9320&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Second, the batch size is only 9, so there is probably not a high rate of upcalls coming in:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;=&gt; Histogram of upcalls per batch: # NumSamples = 61069; Min = 1; Max = 9 # each ∎ represents a count of 14575 1 [ 58301]: ∎∎∎∎ 33 [ 0]: 2 [ 2052]: 34 [ 0]: 3 [ 451]: 35 [ 0]: 4 [ 167]: 36 [ 0]: 5 [ 62]: 37 [ 0]: 6 [ 23]: 38 [ 0]: 7 [ 10]: 39 [ 0]: 8 [ 2]: 40 [ 0]: 9 [ 1]: 41 [ 0]: 10 [ 0]: 42 [ 0]: 11 [ 0]: 43 [ 0]: 12 [ 0]: 44 [ 0]: 13 [ 0]: 45 [ 0]: 14 [ 0]: 46 [ 0]: 15 [ 0]: 47 [ 0]: 16 [ 0]: 48 [ 0]: 17 [ 0]: 49 [ 0]: 18 [ 0]: 50 [ 0]: 19 [ 0]: 51 [ 0]: 20 [ 0]: 52 [ 0]: 21 [ 0]: 53 [ 0]: 22 [ 0]: 54 [ 0]: 23 [ 0]: 55 [ 0]: 24 [ 0]: 56 [ 0]: 25 [ 0]: 57 [ 0]: 26 [ 0]: 58 [ 0]: 27 [ 0]: 59 [ 0]: 28 [ 0]: 60 [ 0]: 29 [ 0]: 61 [ 0]: 30 [ 0]: 62 [ 0]: 31 [ 0]: 63 [ 0]: 32 [ 0]: 64 [ 0]:&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following four histograms show various measurements of time spent processing various parts of the upcalls. The last two show the difference between the total time and the total time minus the flow lookup time.&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;=&gt; Kernel upcall action to vswitchd receive (microseconds): # NumSamples = 57538; Min = 5.28; Max = 1017.34 # Mean = 37.127455; Variance = 449.345903; SD = 21.197781; Median 28.918000 # each ∎ represents a count of 591 5.2830 - 55.8858 [ 44328]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 55.8858 - 106.4885 [ 13091]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 106.4885 - 157.0912 [ 89]: 157.0912 - 207.6940 [ 7]: 207.6940 - 258.2968 [ 5]: 258.2968 - 308.8995 [ 3]: 308.8995 - 359.5023 [ 5]: 359.5023 - 410.1050 [ 2]: 410.1050 - 460.7078 [ 4]: 460.7078 - 511.3105 [ 0]: 511.3105 - 561.9133 [ 0]: 561.9133 - 612.5160 [ 0]: 612.5160 - 663.1187 [ 0]: 663.1187 - 713.7215 [ 0]: 713.7215 - 764.3243 [ 1]: 764.3243 - 814.9270 [ 0]: 814.9270 - 865.5298 [ 2]: 865.5298 - 916.1325 [ 0]: 916.1325 - 966.7353 [ 0]: 966.7353 - 1017.3380 [ 1]:&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="java"&gt;=&gt; vswitchd execute to kernel receive (microseconds): # NumSamples = 56446; Min = 2.34; Max = 991.14 # Mean = 25.006876; Variance = 186.347754; SD = 13.650925; Median 24.358000 # each ∎ represents a count of 732 2.3440 - 51.7839 [ 54970]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 51.7839 - 101.2239 [ 1394]: ∎ 101.2239 - 150.6638 [ 71]: 150.6638 - 200.1038 [ 5]: 200.1038 - 249.5437 [ 2]: 249.5437 - 298.9837 [ 0]: 298.9837 - 348.4236 [ 0]: 348.4236 - 397.8636 [ 0]: 397.8636 - 447.3035 [ 0]: 447.3035 - 496.7435 [ 0]: 496.7435 - 546.1834 [ 1]: 546.1834 - 595.6234 [ 0]: 595.6234 - 645.0634 [ 0]: 645.0634 - 694.5033 [ 0]: 694.5033 - 743.9433 [ 0]: 743.9433 - 793.3832 [ 0]: 793.3832 - 842.8231 [ 0]: 842.8231 - 892.2631 [ 0]: 892.2631 - 941.7030 [ 1]: 941.7030 - 991.1430 [ 2]:&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="java"&gt;=&gt; Upcall overhead (total time minus lookup) (microseconds): # NumSamples = 51329; Min = 18.45; Max = 1051.58 # Mean = 66.283183; Variance = 748.900766; SD = 27.366051; Median 58.436000 # each ∎ represents a count of 442 18.4520 - 70.1084 [ 33178]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 70.1084 - 121.7647 [ 17153]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 121.7647 - 173.4210 [ 927]: ∎∎ 173.4210 - 225.0774 [ 50]: 225.0774 - 276.7337 [ 6]: 276.7337 - 328.3901 [ 1]: 328.3901 - 380.0464 [ 2]: 380.0464 - 431.7028 [ 1]: 431.7028 - 483.3591 [ 2]: 483.3591 - 535.0155 [ 1]: 535.0155 - 586.6718 [ 1]: 586.6718 - 638.3282 [ 0]: 638.3282 - 689.9845 [ 0]: 689.9845 - 741.6409 [ 0]: 741.6409 - 793.2972 [ 1]: 793.2972 - 844.9536 [ 0]: 844.9536 - 896.6099 [ 1]: 896.6099 - 948.2663 [ 1]: 948.2663 - 999.9226 [ 2]: 999.9226 - 1051.5790 [ 2]:&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="java"&gt;=&gt; Kernel upcall to kernel packet execute (microseconds): # NumSamples = 56446; Min = 19.57; Max = 1306.49 # Mean = 149.825728; Variance = 4282.641028; SD = 65.441890; Median 144.253500 # each ∎ represents a count of 276 19.5700 - 83.9161 [ 8637]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 83.9161 - 148.2623 [ 20707]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 148.2623 - 212.6084 [ 18989]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 212.6084 - 276.9546 [ 6135]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 276.9546 - 341.3007 [ 1431]: ∎∎∎∎∎ 341.3007 - 405.6469 [ 380]: ∎ 405.6469 - 469.9930 [ 101]: 469.9930 - 534.3392 [ 34]: 534.3392 - 598.6853 [ 14]: 598.6853 - 663.0315 [ 6]: 663.0315 - 727.3777 [ 2]: 727.3777 - 791.7238 [ 0]: 791.7238 - 856.0699 [ 2]: 856.0699 - 920.4161 [ 2]: 920.4161 - 984.7622 [ 1]: 984.7622 - 1049.1084 [ 1]: 1049.1084 - 1113.4545 [ 2]: 1113.4545 - 1177.8007 [ 1]: 1177.8007 - 1242.1468 [ 0]: 1242.1468 - 1306.4930 [ 0]:&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Figure 1 shows the spread of times as boxplots, with averages marked as red triangles and the medians as vertical orange lines. The values in the Total boxplot came from the "Kernel upcall to kernel packet execute (microseconds)" histogram shown earlier. The values in the Overhead boxplot came from the "Upcall overhead (total time minus lookup) (microseconds)" histogram and represent the time taken by the &lt;code&gt;dpif_*&lt;/code&gt; calls. The mean total is 150 microseconds, and the mean overhead is 66, or 44% of the total. The median is similar: 144 total and 58 overhead, or 40% of the total.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/f1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/f1.png?itok=30_yam2e" width="1280" height="480" alt="Overhead averages out to 40% to 44% of the total. See the text for details." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. Overhead averages out to 40% to 44% of the total. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Although the overheads in Figure 1 look high, when you consider the small amount of absolute time required and know that the sequences could potentially do three kernel interactions, the overhead does not look too bad. Of course, there are outliers, but the script in the current form does not report any statistics on those.&lt;/p&gt; &lt;h2 id="looking-at-some-fabricated-data"&gt;Runs with test data&lt;/h2&gt; &lt;p&gt;In the lab, we do a lot of benchmarking, which in most cases does not use a realistic traffic pattern. One of those tests is physical to virtual to physical (PVP), where we send a high amount of traffic, mostly at wire speed, toward the device under test (DUT) and measure the throughput. This high rate of unknown traffic puts a lot of stress on the lookup part of OVS. One catch is that packets are queued to userspace until the flow gets installed, which at wire speed leads to buffer overruns and in turn lost packets.&lt;/p&gt; &lt;p&gt;Here is an example of running an &lt;a href="https://github.com/chaudron/ovs_perf"&gt;ovs_perf&lt;/a&gt; test with 64-byte packets and a total of 100 flows:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# upcall_cost.py -w pvp_64_batch64_full_20sec_random_100flows.cost -q --buffer-page-count 64000 - Compiling eBPF programs... - Capturing events [Press ^C to stop]... ^C - Analyzing results (56316 events)... =&gt; Events received per type (usable/total) [missed events]: dpif_netlink_operate__op_flow_execute : 13835/ 13835 [ 0] dpif_netlink_operate__op_flow_put : 200/ 200 [ 0] dpif_recv__recv_upcall : 13835/ 13835 [ 0] ktrace__ovs_packet_cmd_execute : 13835/ 13835 [ 0] openvswitch__dp_upcall : 14611/ 14611 [ 0]&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The first thing to notice here is the imbalance between the 14,611 events handled by &lt;code&gt;openvswitch__dp_upcall&lt;/code&gt; and the 13,835 events handled by &lt;code&gt;dpif_recv__recv_upcall&lt;/code&gt; and related calls. This means that 776 upcalls failed to be sent to userspace.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: I had to modify the &lt;code&gt;ovs_pvp&lt;/code&gt; script to send random packet data, or else the packet would be matched wrongly; i.e., a missed &lt;code&gt;openvswitch__dp_upcall&lt;/code&gt; would match to the first identical packet received by &lt;code&gt;dpif_recv__recv_upcall&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Further output shows that 98% of the packets were batched together in batches of 64 packets:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;- Analyzing 13807 event sets... ... =&gt; Histogram of upcalls per batch: # NumSamples = 225; Min = 1; Max = 64 # each ∎ represents a count of 52 1 [ 2]: 33 [ 0]: 2 [ 0]: 34 [ 0]: 3 [ 0]: 35 [ 1]: 4 [ 0]: 36 [ 1]: 5 [ 0]: 37 [ 0]: 6 [ 0]: 38 [ 0]: 7 [ 0]: 39 [ 0]: 8 [ 1]: 40 [ 0]: 9 [ 0]: 41 [ 0]: 10 [ 0]: 42 [ 0]: 11 [ 0]: 43 [ 1]: 12 [ 0]: 44 [ 0]: 13 [ 0]: 45 [ 0]: 14 [ 0]: 46 [ 0]: 15 [ 0]: 47 [ 0]: 16 [ 0]: 48 [ 0]: 17 [ 1]: 49 [ 0]: 18 [ 1]: 50 [ 0]: 19 [ 1]: 51 [ 0]: 20 [ 0]: 52 [ 0]: 21 [ 0]: 53 [ 0]: 22 [ 2]: 54 [ 0]: 23 [ 0]: 55 [ 0]: 24 [ 0]: 56 [ 1]: 25 [ 0]: 57 [ 0]: 26 [ 1]: 58 [ 0]: 27 [ 1]: 59 [ 0]: 28 [ 0]: 60 [ 0]: 29 [ 0]: 61 [ 0]: 30 [ 0]: 62 [ 0]: 31 [ 0]: 63 [ 0]: 32 [ 0]: 64 [ 211]: ∎∎∎∎&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A histogram of the total time follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;=&gt; Kernel upcall to kernel packet execute (microseconds): # NumSamples = 13807; Min = 622.86; Max = 100147.68 # Mean = 29367.911161; Variance = 810227590.080283; SD = 28464.497011; Median 17470.188000 # each ∎ represents a count of 51 622.8620 - 5599.1031 [ 3834]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 5599.1031 - 10575.3443 [ 1209]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 10575.3443 - 15551.5854 [ 1264]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 15551.5854 - 20527.8266 [ 1036]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 20527.8266 - 25504.0678 [ 550]: ∎∎∎∎∎∎∎∎∎∎ 25504.0678 - 30480.3089 [ 640]: ∎∎∎∎∎∎∎∎∎∎∎∎ 30480.3089 - 35456.5500 [ 576]: ∎∎∎∎∎∎∎∎∎∎∎ 35456.5500 - 40432.7912 [ 576]: ∎∎∎∎∎∎∎∎∎∎∎ 40432.7912 - 45409.0324 [ 576]: ∎∎∎∎∎∎∎∎∎∎∎ 45409.0324 - 50385.2735 [ 576]: ∎∎∎∎∎∎∎∎∎∎∎ 50385.2735 - 55361.5146 [ 192]: ∎∎∎ 55361.5146 - 60337.7558 [ 315]: ∎∎∎∎∎∎ 60337.7558 - 65313.9970 [ 261]: ∎∎∎∎∎ 65313.9970 - 70290.2381 [ 384]: ∎∎∎∎∎∎∎ 70290.2381 - 75266.4792 [ 320]: ∎∎∎∎∎∎ 75266.4792 - 80242.7204 [ 256]: ∎∎∎∎∎ 80242.7204 - 85218.9615 [ 320]: ∎∎∎∎∎∎ 85218.9615 - 90195.2027 [ 320]: ∎∎∎∎∎∎ 90195.2027 - 95171.4438 [ 262]: ∎∎∎∎∎ 95171.4438 - 100147.6850 [ 340]: ∎∎∎∎∎∎&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A histogram of the same events follows with the batch size forced down to a single packet:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;=&gt; Kernel upcall to kernel packet execute (microseconds): # NumSamples = 14861; Min = 31.18; Max = 170137.33 # Mean = 41191.399403; Variance = 1889961006.419044; SD = 43473.681767; Median 21085.577000 # each ∎ represents a count of 41 31.1810 - 8536.4883 [ 3019]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 8536.4883 - 17041.7955 [ 3139]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 17041.7955 - 25547.1028 [ 1880]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 25547.1028 - 34052.4100 [ 841]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 34052.4100 - 42557.7172 [ 997]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 42557.7172 - 51063.0245 [ 937]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 51063.0245 - 59568.3317 [ 888]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 59568.3317 - 68073.6390 [ 134]: ∎∎∎ 68073.6390 - 76578.9462 [ 331]: ∎∎∎∎∎∎∎∎ 76578.9462 - 85084.2535 [ 294]: ∎∎∎∎∎∎∎ 85084.2535 - 93589.5607 [ 233]: ∎∎∎∎∎ 93589.5607 - 102094.8680 [ 238]: ∎∎∎∎∎ 102094.8680 - 110600.1753 [ 249]: ∎∎∎∎∎∎ 110600.1753 - 119105.4825 [ 230]: ∎∎∎∎∎ 119105.4825 - 127610.7897 [ 223]: ∎∎∎∎∎ 127610.7897 - 136116.0970 [ 230]: ∎∎∎∎∎ 136116.0970 - 144621.4043 [ 273]: ∎∎∎∎∎∎ 144621.4043 - 153126.7115 [ 248]: ∎∎∎∎∎∎ 153126.7115 - 161632.0188 [ 230]: ∎∎∎∎∎ 161632.0188 - 170137.3260 [ 247]: ∎∎∎∎∎∎&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I had expected that the maximum time for a single packet batch would be less than it was for the 64-packet batch, because I removed the &lt;em&gt;relatively&lt;/em&gt; long delay of doing 64 flow lookups before the packets get sent out. However, I guess that the stress on the kernel—which these statistics don't show—could make up for the time saved.&lt;/p&gt; &lt;p&gt;Figure 2 was created using the same method as before, but instead of continuously sending packets at wire speed, we sent in 1,000 packets/flows and stopped. That is, OVS does just the learning part. In addition, we set the number of handler threads to 1. Because of the wide ranges, we represented the microseconds on the X axis exponentially. This is why the averages in the boxplots appear skewed toward the right.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/f2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/f2.png?itok=lLkZiNGM" width="1280" height="480" alt="The spread of execution times and the averages shift for different batch sizes, with medians showing very different results from means. See the text for details." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. The spread of execution times and the averages shift for different batch sizes, with medians showing very different results from means. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Smaller batch sizes enjoyed lower median execution times. In other words, 50% of the batch sizes are handled faster, but others take longer. This is probably because decreasing the receiving batch size also decreases the batching of the flow addition and execution messages toward the kernel. In addition, the total time per flow also increases over time, because we sent in more frames than the CPU can handle.&lt;/p&gt; &lt;p&gt;To alleviate this CPU load, we tried one more test that sent 100 packets, physical to virtual (PV) only. Figure 3 shows the results.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/f3.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/f3.png?itok=Gcml8ENJ" width="1280" height="480" alt="Running a stress test without extra CPU overload shows that 32-packet batches perform slightly better than 64-packet batches." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3. Different times are reported by a stress test without extra CPU overload. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Whereas Figure 2 showed that a batch size of 64 offers the best mean and median performance, Figure 3 suggests that a batch size of 32 is better. These measurements include the total time it takes before all the flows are programed in the kernel module, when no upcalls are happening.&lt;/p&gt; &lt;p&gt;We could go on forever looking at more data, such as the total time it takes before all packets are handled by the kernel, the frequency and spread of the upcalls, etc. We could look at just the total time it costs to process the lookup by removing the &lt;code&gt;openvswitch__dp_upcall&lt;/code&gt; from the set. This change would probably give better results for smaller batch sizes, but output from that run of the script would not show the longer time it takes to program all flows in the system, and thus would not show the complete picture of a system under stress.&lt;/p&gt; &lt;p&gt;My sample runs have shown what I want to emphasize in this article: The way the upcalls are generated highly influences the outcome of the upcall costs.&lt;/p&gt; &lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article has shown the value of measuring the cost of upcalls, giving you good insight into your system's behavior. With the &lt;code&gt;upcall_cost.py&lt;/code&gt; script available, monitoring OVS overhead is now possible on a fine-grained level.&lt;/p&gt; &lt;p&gt;Upcall costs differ between an average system and an overloaded one. It would be of interest to the community to capture and share upcall statistics. With this data, we could further tailor OVS to better behave in the most common scenarios. Try the &lt;code&gt;upcall_cost.py&lt;/code&gt; script on your systems and work with us.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/02/07/investigating-cost-open-vswitch-upcalls-linux" title="Investigating the cost of Open vSwitch upcalls in Linux"&gt;Investigating the cost of Open vSwitch upcalls in Linux&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Eelco Chaudron</dc:creator><dc:date>2022-02-07T07:00:00Z</dc:date></entry><entry><title type="html">Reverse engineer your JBoss AS-WildFly configuration to CLI</title><link rel="alternate" href="http://www.mastertheboss.com/jbossas/jboss-script/reverse-engineer-your-jboss-as-wildfly-configuration-to-cli/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/jbossas/jboss-script/reverse-engineer-your-jboss-as-wildfly-configuration-to-cli/</id><updated>2022-02-07T02:12:00Z</updated><content type="html">Exporting your WildFly / JBoss EAP configuration to a CLI script is something you are going to need one day or another. No worries, the project Profile Cloner comes to rescue! (Updated to work with WildFly 26 and Java 11) I’ve cloned ProfileCloner! The original  Profile Cloner Project allows to create a clone of your ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">Quarkus World Tour 2022 - The Road Never Ends</title><link rel="alternate" href="https://quarkus.io/blog/quarkusworldtour-2022/" /><author><name>James Cobb</name></author><id>https://quarkus.io/blog/quarkusworldtour-2022/</id><updated>2022-02-04T00:00:00Z</updated><content type="html">For true rock stars, the road never ends. We’re bringing the band back on the road to continue bringing our unique, hands-on experience with access to Quarkus experts designed to help you get started with Java in a Kubernetes world. Whether you are a Quarkus first-timer or seasoned pro, your...</content><dc:creator>James Cobb</dc:creator></entry><entry><title type="html">Running Arquillian Tests from Eclipse</title><link rel="alternate" href="http://www.mastertheboss.com/jboss-frameworks/arquillian/running-arquillian-tests-from-eclipse/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/jboss-frameworks/arquillian/running-arquillian-tests-from-eclipse/</id><updated>2022-02-03T10:55:33Z</updated><content type="html">Have you ever wondered how to run Arquillian Integration tests without leaving your favourite IDE? Then keep reading! For the purpose of this example, we will be using Eclipse, however you can easily adapt it to any other IDE such as Idea or Visual Studio. An Arquillian Project to Test Our short demo will be ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>Build a REST API from the ground up with Quarkus 2.0</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/02/03/build-rest-api-ground-quarkus-20" /><author><name>Stephen Nimmo</name></author><id>e352281d-2160-476d-8ca9-37a002c9379b</id><updated>2022-02-03T07:00:00Z</updated><published>2022-02-03T07:00:00Z</published><summary type="html">&lt;p&gt;It's been almost a year since Red Hat Developer published &lt;a href="https://developers.redhat.com/blog/2021/05/11/building-an-api-using-quarkus-from-the-ground-up"&gt;Build an API using Quarkus from the ground up&lt;/a&gt;. That article tried to provide a single full reference implementation of an OpenAPI-compliant REST API using &lt;a href="https://developers.redhat.com/products/quarkus/overview"&gt;Quarkus&lt;/a&gt;. Since then, there's been a &lt;a href="https://quarkus.io/blog/quarkus-2-0-0-final-released/"&gt;major version release&lt;/a&gt; of Quarkus, with new features that make building and maintaining a Quarkus-based REST API much easier. In this article, you will revisit the Customer API from the previous article and see how it can be improved thanks to advances in Quarkus.&lt;/p&gt; &lt;p&gt;In creating this article, we made an effort to remain aware of a subtle consideration in any development effort: you always need to keep an eye on your imports. Whenever you add imports, you should consciously attempt to limit your exposure to third-party libraries, focusing on staying in the abstraction layers such as the MicroProfile abstractions. Remember, every library you import is your responsibility to care for and update.&lt;/p&gt; &lt;p&gt;You can find the &lt;a href="https://github.com/quarkus-ground-up/customer-api/tree/0.0.1"&gt;source code for this article on GitHub&lt;/a&gt;. You should familiarize yourself with the original version of the Customer API by reading the earlier article before you begin this one.&lt;/p&gt; &lt;h2&gt;Architecture layers: Resource, service, and repository&lt;/h2&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Diagram of the architectural layers: Resource, Service and Repository" data-entity-type="file" data-entity-uuid="d42f5b2d-28e7-4888-93e5-70e093ae8272" src="https://developers.redhat.com/sites/default/files/inline-images/img_resource_layers.png" width="600" height="210" loading="lazy" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 1. Architectural layers for the typical REST API.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The architecture layers of the Customer API haven't changed, as you can see in Figure 1. In a rapid development effort, some developers might think these layers are cumbersome and unnecessary for a prototype or minimum viable product. Using the &lt;a href="https://martinfowler.com/bliki/Yagni.html"&gt;YAGNI&lt;/a&gt; ("You aren't gonna need it") approach has its benefits. In my experience working in enterprise environments, however, the YAGNI approach can come back to haunt a development team. There's a difference between development using best practices and premature optimization, but the dichotomy can be highly subjective. Be mindful of perceived expectations regarding what a prototype means to the business in terms of being production-ready.&lt;/p&gt; &lt;h2&gt;Project initialization&lt;/h2&gt; &lt;p&gt;Begin where you started last time: at the command line.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;mvn io.quarkus.platform:quarkus-maven-plugin:2.6.1.Final:create \ -DprojectGroupId=com.redhat.customer \ -DprojectArtifactId=customer-api \ -DclassName="com.redhat.customer.CustomerResource" \ -Dpath="/customers" \ -Dextensions="config-yaml,resteasy-reactive-jackson" cd customer-api&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Right off the bat, you'll notice we've jumped from &lt;code&gt;1.13.3&lt;/code&gt; to &lt;code&gt;2.6.1&lt;/code&gt;. That's a pretty big jump, and with it comes a bevy of new features and functionality.&lt;/p&gt; &lt;p&gt;Quarkus added &lt;a href="https://quarkus.io/guides/dev-services"&gt;Dev Services&lt;/a&gt; into the framework, which use &lt;a href="https://www.testcontainers.org"&gt;Testcontainers&lt;/a&gt; behind the scenes to spin up the necessary infrastructure to perform integration testing in a local environment with production-like services. &lt;a href="https://developers.redhat.com/topics/java"&gt;Java&lt;/a&gt; 11 is now the new standard, along with GraalVM 21, Vert.x 4, and Eclipse MicroProfile 4. There's also a new feature for continuous testing, which allows tests to be executed in a parallel process alongside an application running in Dev Mode. The continuous testing feature is a huge gain in development productivity, giving you fast feedback to changes that might break the tests.&lt;/p&gt; &lt;h3&gt;Lombok and MapStruct&lt;/h3&gt; &lt;p&gt;&lt;a href="https://projectlombok.org/"&gt;Lombok&lt;/a&gt; is a &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; library that helps keep the amount of boilerplate code in an application to a minimum; however, its use is not without controversy in the developer community. There are plenty of articles out there detailing the good and bad sides of using code generation annotations. You will be using it for this application, but as with any library you import, remember that it's up to you and your team to understand its implications.&lt;/p&gt; &lt;p&gt;&lt;a href="https://mapstruct.org/"&gt;MapStruct&lt;/a&gt; is a code generator that greatly simplifies the implementation of mappings between JavaBean types based on a convention-over-configuration approach. It seems to be less controversial than Lombok; however, it's still doing code generation, so the same caution should be used.&lt;/p&gt; &lt;p&gt;When you're using the two in combination, it's important to have both the Lombok annotation processor and the MapStruct annotation processor configured in the compiler plugin to ensure that both are executed. Here are the excerpts of the &lt;code&gt;pom.xml&lt;/code&gt; changes you'll need to make.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-xml"&gt;&lt;properties&gt; ... &lt;lombok.version&gt;1.18.22&lt;/lombok.version&gt; &lt;mapstruct.version&gt;1.4.2.Final&lt;/mapstruct.version&gt; ... &lt;/properties&gt; &lt;dependencies&gt; ... &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;${lombok.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mapstruct&lt;/groupId&gt; &lt;artifactId&gt;mapstruct&lt;/artifactId&gt; &lt;version&gt;${mapstruct.version}&lt;/version&gt; &lt;/dependency&gt; ... &lt;dependencies&gt; &lt;build&gt; &lt;plugins&gt; ... &lt;plugin&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;${compiler-plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;compilerArgs&gt; &lt;arg&gt;-parameters&lt;/arg&gt; &lt;/compilerArgs&gt; &lt;annotationProcessorPaths&gt; &lt;path&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;${lombok.version}&lt;/version&gt; &lt;/path&gt; &lt;path&gt; &lt;groupId&gt;org.mapstruct&lt;/groupId&gt; &lt;artifactId&gt;mapstruct-processor&lt;/artifactId&gt; &lt;version&gt;${mapstruct.version}&lt;/version&gt; &lt;/path&gt; &lt;/annotationProcessorPaths&gt; &lt;/configuration&gt; &lt;/plugin&gt; ... &lt;/plugins&gt; &lt;/build&gt;&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Exceptions&lt;/h3&gt; &lt;p&gt;Next, you'll add a quick exception implementation for use in the project. By using a simple &lt;code&gt;ServiceException&lt;/code&gt;, you can simplify the exception model and add some message formatting for ease of use in the code.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;package com.redhat.exception; public class ServiceException extends RuntimeException { public ServiceException(String message) { super(message); } public ServiceException(String format, Object... objects) { super(String.format(format, objects)); } }&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Repository layer&lt;/h2&gt; &lt;p&gt;Since we are starting from the ground up, it's time to talk database. Database interactions are going to be managed by the &lt;a href="https://quarkus.io/guides/hibernate-orm-panache"&gt;Quarkus Panache&lt;/a&gt; extension. The Panache extension uses &lt;a href="https://hibernate.org"&gt;Hibernate&lt;/a&gt; under the covers, but provides a ton of functionality on top to make developers more productive. The demo will use a &lt;a href="https://www.postgresql.org"&gt;PostgreSQL&lt;/a&gt; database and will manage the schema using &lt;a href="https://flywaydb.org"&gt;Flyway&lt;/a&gt;, which allows database schemas to be versioned and managed in source control. You'll add the &lt;a href="https://hibernate.org/validator/"&gt;Hibernate validator&lt;/a&gt; extension as well because validation annotations will be used across all your &lt;a href="https://en.wikipedia.org/wiki/Plain_old_Java_object"&gt;plain old Java objects&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Add these extensions to the project's &lt;code&gt;pom.xml&lt;/code&gt; file.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-xml"&gt;&lt;dependency&gt; &lt;groupId&gt;io.quarkus&lt;/groupId&gt; &lt;artifactId&gt;quarkus-hibernate-validator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.quarkus&lt;/groupId&gt; &lt;artifactId&gt;quarkus-hibernate-orm-panache&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.quarkus&lt;/groupId&gt; &lt;artifactId&gt;quarkus-jdbc-postgresql&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.quarkus&lt;/groupId&gt; &lt;artifactId&gt;quarkus-flyway&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Flyway&lt;/h3&gt; &lt;p&gt;Next, you'll put together the &lt;code&gt;customer&lt;/code&gt; table. Flyway &lt;a href="https://flywaydb.org/documentation/getstarted/firststeps/maven#creating-the-first-migration"&gt;expects&lt;/a&gt; the migration files to be located in a folder on your classpath named &lt;code&gt;db/migration&lt;/code&gt;. Here's how you'd create the first version of the customer table:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; src/main/resources/db/migration/V1__customer_table_create.sql &lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-sql"&gt;CREATE TABLE customer ( customer_id SERIAL PRIMARY KEY, first_name TEXT NOT NULL, middle_name TEXT, last_name TEXT NOT NULL, suffix TEXT, email TEXT, phone TEXT ); ALTER SEQUENCE customer_customer_id_seq RESTART 1000000;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This table is different in a couple of subtle ways from its equivalent in the previous article. First, the &lt;code&gt;VARCHAR(100)&lt;/code&gt; columns have been replaced with &lt;code&gt;TEXT&lt;/code&gt; columns. A &lt;code&gt;TEXT&lt;/code&gt; column is a variable-length string column with unlimited length. According to the &lt;a href="https://www.postgresql.org/docs/current/datatype-character.html"&gt;PostgreSQL documentation&lt;/a&gt;, making this change does not have any performance impacts. Why keep an arbitrary limit on column sizes when getting rid of it doesn't cost anything?&lt;/p&gt; &lt;p&gt;The new table also includes an &lt;code&gt;ALTER&lt;/code&gt; statement to adjust the starting point for the generated &lt;code&gt;SEQUENCE&lt;/code&gt; created from the &lt;code&gt;SERIAL&lt;/code&gt; type. By starting at 1 million, you can actually use key values smaller than the starting point to insert baseline test data, which might come in handy in your testing scenarios. Of course, doing this "wastes" a million IDs, but the &lt;code&gt;SERIAL&lt;/code&gt; type is an integer capable of holding 2,147,483,647 unique values. This gives you a huge runway.&lt;/p&gt; &lt;h3&gt;JPA with Panache&lt;/h3&gt; &lt;p&gt;Using the Java Persistence API (JPA) begins with building an entity object. When using Panache, you have a choice between two patterns: &lt;a href="https://quarkus.io/guides/hibernate-orm-panache#solution-1-using-the-active-record-pattern"&gt;Active Record&lt;/a&gt; and &lt;a href="https://quarkus.io/guides/hibernate-orm-panache#solution-2-using-the-repository-pattern"&gt;Repository&lt;/a&gt;. I prefer the Repository pattern because I tend to favor the &lt;a href="https://en.wikipedia.org/wiki/Single-responsibility_principle"&gt;single-responsibility principle&lt;/a&gt;, and the Active Record pattern blends the querying actions with the data.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;package com.redhat.customer; import lombok.Data; import javax.persistence.*; import javax.validation.constraints.Email; import javax.validation.constraints.NotEmpty; @Entity(name = "Customer") @Table(name = "customer") @Data public class CustomerEntity { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) @Column(name = "customer_id") private Integer customerId; @Column(name = "first_name") @NotEmpty private String firstName; @Column(name = "middle_name") private String middleName; @Column(name = "last_name") @NotEmpty private String lastName; @Column(name = "suffix") private String suffix; @Column(name = "email") @Email private String email; @Column(name = "phone") private String phone; }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;CustomerEntity&lt;/code&gt; hasn't changed from the previous incarnation of the Customer API. The only difference is the use of the &lt;a href="https://projectlombok.org/features/Data"&gt;&lt;code&gt;@Data&lt;/code&gt; annotation from Lombok&lt;/a&gt; to handle the generation of the &lt;code&gt;getters/setters/hashCode/equals/toString&lt;/code&gt;. The practices detailed in the previous article still hold true:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;I like to name all my JPA entity classes with the suffix &lt;code&gt;Entity&lt;/code&gt;. They serve a purpose: to map back to the database tables. I always provide a layer of indirection between &lt;code&gt;Domain&lt;/code&gt; objects and &lt;code&gt;Entity&lt;/code&gt; objects because when it's missing, I've lost more time than I've spent creating and managing the data copying processes.&lt;/li&gt; &lt;li&gt;Because of the way the JPA creates the target object names, you have to explicitly put in the &lt;code&gt;@Entity&lt;/code&gt; annotation with the name of the entity you want so your HQL queries don't have to reference &lt;code&gt;CustomerEntity&lt;/code&gt;. Using the &lt;code&gt;@Entity&lt;/code&gt; specifying the name attribute allows you to use the name &lt;code&gt;Customer&lt;/code&gt; rather than the class name.&lt;/li&gt; &lt;li&gt;I like to explicitly name both the table and the columns with the &lt;code&gt;@Table&lt;/code&gt; and &lt;code&gt;@Column&lt;/code&gt; annotations. Why? I've lost more time when a code refactor inadvertently breaks the assumed named contracts than the time it costs to write a few extra annotations.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The next step is to create the &lt;code&gt;Repository&lt;/code&gt; class:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;package com.redhat.customer; import io.quarkus.hibernate.orm.panache.PanacheRepositoryBase; import javax.enterprise.context.ApplicationScoped; @ApplicationScoped public class CustomerRepository implements PanacheRepositoryBase&lt;CustomerEntity, Integer&gt; { }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The Panache library adds value by providing the most widely used set of query methods out of the box, as you can see in Figure 2.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Screenshot of Panache methods available to the developer" data-entity-type="file" data-entity-uuid="fd83e12e-81c3-4324-8e73-15e6ed473ca4" src="https://developers.redhat.com/sites/default/files/inline-images/panache_methods.png" width="700" height="595" loading="lazy" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 2. The Panache repository provides a myriad of methods for developers.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Service layer: Domain object, MapStruct mapper, and Service&lt;/h2&gt; &lt;p&gt;In this example, the domain object is fairly simple. It's basically a copy of the entity object with the persistence annotations ripped out. As previously mentioned, this might seem like overkill, as the entity object in most basic tutorials tends to just get passed up through the layers. However, if at some point in the future there were a potential divergence between how the data is stored in the database and how the domain is modeled, this additional layer would be of benefit. It takes a bit of work to create the layering now, but that keeps the architecture clean, and it's not worth it to defer the work and restrict the dependencies to their respective layers.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;package com.redhat.customer; import lombok.Data; import javax.validation.constraints.Email; import javax.validation.constraints.NotEmpty; @Data public class Customer { private Integer customerId; @NotEmpty private String firstName; private String middleName; @NotEmpty private String lastName; private String suffix; @Email private String email; private String phone; }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the service layer, you will need to convert entity objects to domain objects. That's where MapStruct comes in: to do the mapping for you.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;package com.redhat.customer; import org.mapstruct.InheritInverseConfiguration; import org.mapstruct.Mapper; import org.mapstruct.MappingTarget; import java.util.List; @Mapper(componentModel = "cdi") public interface CustomerMapper { List&lt;Customer&gt; toDomainList(List&lt;CustomerEntity&gt; entities); Customer toDomain(CustomerEntity entity); @InheritInverseConfiguration(name = "toDomain") CustomerEntity toEntity(Customer domain); void updateEntityFromDomain(Customer domain, @MappingTarget CustomerEntity entity); void updateDomainFromEntity(CustomerEntity entity, @MappingTarget Customer domain); }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now that you have the domain class and the needed MapStruct mapper to convert entity-to-domain and vice versa, you can add the &lt;code&gt;Service&lt;/code&gt; class for the basic CRUD (Create, Read, Update, Delete) functionality.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;package com.redhat.customer; import com.redhat.exception.ServiceException; import lombok.AllArgsConstructor; import lombok.NonNull; import lombok.extern.slf4j.Slf4j; import javax.enterprise.context.ApplicationScoped; import javax.transaction.Transactional; import javax.validation.Valid; import java.util.List; import java.util.Objects; import java.util.Optional; @ApplicationScoped @AllArgsConstructor @Slf4j public class CustomerService { private final CustomerRepository customerRepository; private final CustomerMapper customerMapper; public List&lt;Customer&gt; findAll() { return this.customerMapper.toDomainList(customerRepository.findAll().list()); } public Optional&lt;Customer&gt; findById(@NonNull Integer customerId) { return customerRepository.findByIdOptional(customerId) .map(customerMapper::toDomain); } @Transactional public void save(@Valid Customer customer) { log.debug("Saving Customer: {}", customer); CustomerEntity entity = customerMapper.toEntity(customer); customerRepository.persist(entity); customerMapper.updateDomainFromEntity(entity, customer); } @Transactional public void update(@Valid Customer customer) { log.debug("Updating Customer: {}", customer); if (Objects.isNull(customer.getCustomerId())) { throw new ServiceException("Customer does not have a customerId"); } CustomerEntity entity = customerRepository.findByIdOptional(customer.getCustomerId()) .orElseThrow(() -&gt; new ServiceException("No Customer found for customerId[%s]", customer.getCustomerId())); customerMapper.updateEntityFromDomain(customer, entity); customerRepository.persist(entity); customerMapper.updateDomainFromEntity(entity, customer); } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Take note of the use of the Lombok annotations &lt;a href="https://projectlombok.org/features/constructor"&gt;&lt;code&gt;@AllArgsConstructor&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://projectlombok.org/features/log"&gt;&lt;code&gt;@Slf4j&lt;/code&gt;&lt;/a&gt;. The &lt;code&gt;@AllArgsConstructor&lt;/code&gt; generates a constructor based on the class variables present and CDI will do the injection automatically. The &lt;code&gt;@Slf4j&lt;/code&gt; annotation generates a &lt;code&gt;log&lt;/code&gt; class variable using the &lt;a href="https://www.slf4j.org/"&gt;Slf4j&lt;/a&gt; library.&lt;/p&gt; &lt;h2&gt;Resource layer&lt;/h2&gt; &lt;p&gt;You are building a REST API, so you need to add the &lt;a href="https://spec.openapis.org/oas/v3.1.0"&gt;OpenAPI&lt;/a&gt; spec to the project. The &lt;code&gt;quarkus-smallrye-openapi&lt;/code&gt; extension brings in the &lt;a href="https://github.com/eclipse/microprofile-open-api/"&gt;MicroProfile OpenAPI&lt;/a&gt; annotations and processor; this helps with the generation of both the OpenAPI schema and the &lt;a href="https://swagger.io/tools/swagger-ui/"&gt;Swagger UI&lt;/a&gt;, which provides a great set of functionality for ad-hoc testing of the API endpoints.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-xml"&gt;&lt;dependency&gt; &lt;groupId&gt;io.quarkus&lt;/groupId&gt; &lt;artifactId&gt;quarkus-smallrye-openapi&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now that the OpenAPI extension is available, you can implement the &lt;code&gt;CustomerResource&lt;/code&gt; class.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;package com.redhat.customer; import com.redhat.exception.ServiceException; import lombok.AllArgsConstructor; import lombok.extern.slf4j.Slf4j; import org.eclipse.microprofile.openapi.annotations.enums.SchemaType; import org.eclipse.microprofile.openapi.annotations.media.Content; import org.eclipse.microprofile.openapi.annotations.media.Schema; import org.eclipse.microprofile.openapi.annotations.parameters.Parameter; import org.eclipse.microprofile.openapi.annotations.responses.APIResponse; import org.eclipse.microprofile.openapi.annotations.tags.Tag; import javax.validation.Valid; import javax.validation.constraints.NotNull; import javax.ws.rs.*; import javax.ws.rs.core.Context; import javax.ws.rs.core.MediaType; import javax.ws.rs.core.Response; import javax.ws.rs.core.UriInfo; import java.net.URI; import java.util.Objects; @Path("/customers") @Produces(MediaType.APPLICATION_JSON) @Consumes(MediaType.APPLICATION_JSON) @Tag(name = "customer", description = "Customer Operations") @AllArgsConstructor @Slf4j public class CustomerResource { private final CustomerService customerService; @GET @APIResponse( responseCode = "200", description = "Get All Customers", content = @Content( mediaType = MediaType.APPLICATION_JSON, schema = @Schema(type = SchemaType.ARRAY, implementation = Customer.class) ) ) public Response get() { return Response.ok(customerService.findAll()).build(); } @GET @Path("/{customerId}") @APIResponse( responseCode = "200", description = "Get Customer by customerId", content = @Content( mediaType = MediaType.APPLICATION_JSON, schema = @Schema(type = SchemaType.OBJECT, implementation = Customer.class) ) ) @APIResponse( responseCode = "404", description = "Customer does not exist for customerId", content = @Content(mediaType = MediaType.APPLICATION_JSON) ) public Response getById(@Parameter(name = "customerId", required = true) @PathParam("customerId") Integer customerId) { return customerService.findById(customerId) .map(customer -&gt; Response.ok(customer).build()) .orElse(Response.status(Response.Status.NOT_FOUND).build()); } @POST @APIResponse( responseCode = "201", description = "Customer Created", content = @Content( mediaType = MediaType.APPLICATION_JSON, schema = @Schema(type = SchemaType.OBJECT, implementation = Customer.class) ) ) @APIResponse( responseCode = "400", description = "Invalid Customer", content = @Content(mediaType = MediaType.APPLICATION_JSON) ) @APIResponse( responseCode = "400", description = "Customer already exists for customerId", content = @Content(mediaType = MediaType.APPLICATION_JSON) ) public Response post(@NotNull @Valid Customer customer, @Context UriInfo uriInfo) { customerService.save(customer); URI uri = uriInfo.getAbsolutePathBuilder().path(Integer.toString(customer.getCustomerId())).build(); return Response.created(uri).entity(customer).build(); } @PUT @Path("/{customerId}") @APIResponse( responseCode = "204", description = "Customer updated", content = @Content( mediaType = MediaType.APPLICATION_JSON, schema = @Schema(type = SchemaType.OBJECT, implementation = Customer.class) ) ) @APIResponse( responseCode = "400", description = "Invalid Customer", content = @Content(mediaType = MediaType.APPLICATION_JSON) ) @APIResponse( responseCode = "400", description = "Customer object does not have customerId", content = @Content(mediaType = MediaType.APPLICATION_JSON) ) @APIResponse( responseCode = "400", description = "Path variable customerId does not match Customer.customerId", content = @Content(mediaType = MediaType.APPLICATION_JSON) ) @APIResponse( responseCode = "404", description = "No Customer found for customerId provided", content = @Content(mediaType = MediaType.APPLICATION_JSON) ) public Response put(@Parameter(name = "customerId", required = true) @PathParam("customerId") Integer customerId, @NotNull @Valid Customer customer) { if (!Objects.equals(customerId, customer.getCustomerId())) { throw new ServiceException("Path variable customerId does not match Customer.customerId"); } customerService.update(customer); return Response.status(Response.Status.NO_CONTENT).build(); } }&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Configuration&lt;/h2&gt; &lt;p&gt;This implementation uses &lt;a href="https://quarkus.io/guides/config-reference#profile-aware-files"&gt;profile-aware files&lt;/a&gt; rather than including all configuration in a single file. The single-file strategy requires the use of profile keys (&lt;code&gt;%dev&lt;/code&gt;, &lt;code&gt;%test&lt;/code&gt;, and so on). If there is a lot of configuration, it can make the file long and difficult to manage. Quarkus now allows for profile-specific configuration files that use the filename pattern &lt;code&gt;application-{profile}.yml&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;For example, the default configurations can be managed in the &lt;code&gt;application.yml&lt;/code&gt; file, but the configuration specific to the dev profile can be placed in a file named &lt;code&gt;application-dev.yml&lt;/code&gt;. The profile files still follow the same rules in terms of overriding and extending the configuration, but clean it up and separate it to make it easier to use.&lt;/p&gt; &lt;p&gt;Here are the four files for the configuration.&lt;/p&gt; &lt;p&gt;&lt;code&gt;application.yml&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt;quarkus: banner: enabled: false hibernate-orm: database: generation: none mp: openapi: extensions: smallrye: info: title: Customer API version: 0.0.1 description: API for retrieving customers contact: email: techsupport@redhat.com name: Customer API Support url: https://github.com/quarkus-ground-up/customer-api license: name: Apache 2.0 url: http://www.apache.org/licenses/LICENSE-2.0.html&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;application-dev.yml&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;quarkus: log: level: INFO category: "com.redhat": level: DEBUG hibernate-orm: log: sql: true flyway: migrate-at-start: true locations: db/migration,db/testdata&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;application-test.yml&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;quarkus: log: level: INFO category: "com.redhat": level: DEBUG hibernate-orm: log: sql: true flyway: migrate-at-start: true locations: db/migration,db/testdata&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;application-prod.yml&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;quarkus: log: level: INFO flyway: migrate-at-start: true locations: db/migration&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The most important variations are in &lt;code&gt;prod&lt;/code&gt;, where the logging is configured differently, and the &lt;code&gt;db/testdata&lt;/code&gt; folder is excluded from the Flyway migrations to make sure test data is not created.&lt;/p&gt; &lt;h2&gt;Testing&lt;/h2&gt; &lt;p&gt;For testing, you will use &lt;a href="https://junit.org/junit5/docs/current/user-guide/"&gt;JUnit 5&lt;/a&gt;, &lt;a href="https://rest-assured.io/"&gt;REST-assured&lt;/a&gt;, and the &lt;a href="https://assertj.github.io/doc/"&gt;AssertJ&lt;/a&gt; libraries. Start by adding those to &lt;code&gt;pom.xml&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-xml"&gt;&lt;dependency&gt; &lt;groupId&gt;io.quarkus&lt;/groupId&gt; &lt;artifactId&gt;quarkus-junit5&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.rest-assured&lt;/groupId&gt; &lt;artifactId&gt;rest-assured&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.assertj&lt;/groupId&gt; &lt;artifactId&gt;assertj-core&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;CustomerResourceTest&lt;/code&gt; class focuses on testing the entire flow for the application.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;package com.redhat.customer; import io.quarkus.test.common.http.TestHTTPEndpoint; import io.quarkus.test.junit.QuarkusTest; import io.restassured.http.ContentType; import org.apache.commons.lang3.RandomStringUtils; import org.junit.jupiter.api.Test; import static io.restassured.RestAssured.given; import static org.assertj.core.api.Assertions.assertThat; @QuarkusTest @TestHTTPEndpoint(CustomerResource.class) public class CustomerResourceTest { @Test public void getAll() { given() .when() .get() .then() .statusCode(200); } @Test public void getById() { Customer customer = createCustomer(); Customer saved = given() .contentType(ContentType.JSON) .body(customer) .post() .then() .statusCode(201) .extract().as(Customer.class); Customer got = given() .when() .get("/{customerId}", saved.getCustomerId()) .then() .statusCode(200) .extract().as(Customer.class); assertThat(saved).isEqualTo(got); } @Test public void getByIdNotFound() { given() .when() .get("/{customerId}", 987654321) .then() .statusCode(404); } @Test public void post() { Customer customer = createCustomer(); Customer saved = given() .contentType(ContentType.JSON) .body(customer) .post() .then() .statusCode(201) .extract().as(Customer.class); assertThat(saved.getCustomerId()).isNotNull(); } @Test public void postFailNoFirstName() { Customer customer = createCustomer(); customer.setFirstName(null); given() .contentType(ContentType.JSON) .body(customer) .post() .then() .statusCode(400); } @Test public void put() { Customer customer = createCustomer(); Customer saved = given() .contentType(ContentType.JSON) .body(customer) .post() .then() .statusCode(201) .extract().as(Customer.class); saved.setFirstName("Updated"); given() .contentType(ContentType.JSON) .body(saved) .put("/{customerId}", saved.getCustomerId()) .then() .statusCode(204); } @Test public void putFailNoLastName() { Customer customer = createCustomer(); Customer saved = given() .contentType(ContentType.JSON) .body(customer) .post() .then() .statusCode(201) .extract().as(Customer.class); saved.setLastName(null); given() .contentType(ContentType.JSON) .body(saved) .put("/{customerId}", saved.getCustomerId()) .then() .statusCode(400); } private Customer createCustomer() { Customer customer = new Customer(); customer.setFirstName(RandomStringUtils.randomAlphabetic(10)); customer.setMiddleName(RandomStringUtils.randomAlphabetic(10)); customer.setLastName(RandomStringUtils.randomAlphabetic(10)); customer.setEmail(RandomStringUtils.randomAlphabetic(10) + "@rhenergy.dev"); customer.setPhone(RandomStringUtils.randomNumeric(10)); return customer; } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A few updates have been made since the previous article around the use of the &lt;code&gt;@TestHTTPEndpoint(CustomerResource.class)&lt;/code&gt; annotation. This annotation reduces the hard-coding of specific endpoint URLs in the tests. For example, instead of having to use the URLs in the &lt;code&gt;http&lt;/code&gt; command (e.g., &lt;code&gt;.get("/customers")&lt;/code&gt;), the annotation allows the developer to simply call &lt;code&gt;.get()&lt;/code&gt;, and the annotation tells the call which resource it's referring to.&lt;/p&gt; &lt;h2&gt;Quarkus Dev Services&lt;/h2&gt; &lt;p&gt;Now that all the code is in place, you're ready to fire it up. From the command line, start the application in Quarkus Dev Mode.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;./mvnw clean quarkus:dev&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The first thing that should jump out at you is that you haven't explicitly set anything up regarding the database. But if you look in the logs, you can see that a database has been set up for you.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;[io.qua.dat.dep.dev.DevServicesDatasourceProcessor] (build-40) Dev Services for the default datasource (postgresql) started.&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The Quarkus Dev Services did that behind the scenes. The database Dev Services will automatically be enabled when a reactive or JDBC datasource extension is present in the application, so long as the database URL has not been configured. The Dev Services ecosystem supports not only databases but a slew of integration products, including AMQP, &lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;Apache Kafka&lt;/a&gt;, MongoDB, Infinispan, and more, with new ones being added frequently.&lt;/p&gt; &lt;h2&gt;Continuous testing&lt;/h2&gt; &lt;p&gt;In addition to the Dev Services, Quarkus also has a new continuous testing capability. When you start the application in Dev Mode, this also starts a separate process in the background to execute the tests. Figure 3 shows the command-line options.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Screenshot showing in-process command-line options" data-entity-type="file" data-entity-uuid="bc8bed59-837a-472d-ac8f-74f3d730a6c0" src="https://developers.redhat.com/sites/default/files/inline-images/Screen%20Shot%202022-01-04%20at%2010.10.29%20AM.png" width="1244" height="200" loading="lazy" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 3. New in-process command-line options.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Press the &lt;code&gt;r&lt;/code&gt; key in the console. The tests should execute in the background, giving you real-time test feedback while running the application in Dev Mode. While the testing process is running, you can actively make code changes, and after every save, the process will detect the changes and the tests will rerun automatically. Talk about real-time feedback! Continuous testing makes it possible to quickly detect breaking tests, and the running server provides the ability to perform ad-hoc tests in parallel.&lt;/p&gt; &lt;h2&gt;Observability&lt;/h2&gt; &lt;p&gt;Quarkus has a number of easy-to-implement features that give you visibility into your application as it's running.&lt;/p&gt; &lt;h3&gt;Health&lt;/h3&gt; &lt;p&gt;One of the greatest things about Quarkus is the ability to get very powerful base functionality out of the box simply by adding an extension. A great example is the &lt;code&gt;quarkus-smallrye-health&lt;/code&gt; extension. Adding this extension to the demo app will illustrate what it can do.&lt;/p&gt; &lt;p&gt;For bonus points, add the extension to the &lt;code&gt;pom.xml&lt;/code&gt; file while the application is running in &lt;code&gt;quarkus:dev&lt;/code&gt; mode. Not only will it import the JAR, but it will detect the change and automatically restart the server. To restart the test process, hit the &lt;code&gt;r&lt;/code&gt; key again and you are back to where you started.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-xml"&gt;&lt;dependency&gt; &lt;groupId&gt;io.quarkus&lt;/groupId&gt; &lt;artifactId&gt;quarkus-smallrye-health&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Importing the &lt;code&gt;smallrye-health&lt;/code&gt; extension directly exposes these REST endpoints:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;/q/health/live&lt;/code&gt;: The application is up and running.&lt;/li&gt; &lt;li&gt;&lt;code&gt;/q/health/ready&lt;/code&gt;: The application is ready to serve requests.&lt;/li&gt; &lt;li&gt;&lt;code&gt;/q/health/started&lt;/code&gt;: The application is started.&lt;/li&gt; &lt;li&gt;&lt;code&gt;/q/health&lt;/code&gt;: Accumulating all health check procedures in the application.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;When you check the &lt;code&gt;/q/health&lt;/code&gt; endpoint, you will also see something you may not have expected: the transitive addition of the &lt;code&gt;quarkus-agroal&lt;/code&gt; extension automatically registers a readiness health check that will validate the datasources.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt;{ "status": "UP", "checks": [ { "name": "Database connections health check", "status": "UP" } ] }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;From here, you are now free to build and configure your own custom health checks &lt;a href="https://quarkus.io/guides/smallrye-health#creating-your-first-health-check"&gt;using the framework&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Metrics&lt;/h3&gt; &lt;p&gt;Metrics are a must-have for any application. It's better to include them early to ensure they're available when you really need them. Luckily, there is a Quarkus extension that makes this easy.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-xml"&gt;&lt;dependency&gt; &lt;groupId&gt;io.quarkus&lt;/groupId&gt; &lt;artifactId&gt;quarkus-micrometer-registry-prometheus&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Dropping in this extension gives you a &lt;a href="https://prometheus.io"&gt;Prometheus&lt;/a&gt;-compatible scraping endpoint with process, JVM, and HTTP statistics out of the box. Go to the &lt;code&gt;/q/metrics &lt;/code&gt;endpoint to check it out. If you want to see the basic HTTP metrics, open the Swagger UI at &lt;code&gt;/q/swagger-ui&lt;/code&gt; and hit a couple of the endpoints to see the resulting metrics.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;... http_server_requests_seconds_count{method="GET",outcome="SUCCESS",status="200",uri="/customers",} 4.0 http_server_requests_seconds_sum{method="GET",outcome="SUCCESS",status="200",uri="/customers",} 0.196336987 http_server_requests_seconds_max{method="GET",outcome="SUCCESS",status="200",uri="/customers",} 0.173861414 ...&lt;/code&gt;&lt;/pre&gt; &lt;h4&gt;Enabling histograms&lt;/h4&gt; &lt;p&gt;The standard metrics are pretty powerful, but a quick configuration change can supercharge them. Most API service-level objectives are built around the &lt;a href="https://en.wikipedia.org/wiki/High_availability#%22Nines%22"&gt;number of nines&lt;/a&gt; of reliability. To quickly enable metrics for the HTTP server requests with percentiles, add the following configuration to the project.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;package com.redhat.configuration; import io.micrometer.core.instrument.Meter; import io.micrometer.core.instrument.config.MeterFilter; import io.micrometer.core.instrument.distribution.DistributionStatisticConfig; import javax.enterprise.context.ApplicationScoped; import javax.enterprise.inject.Produces; public class MicrometerConfiguration { @Produces @ApplicationScoped public MeterFilter enableHistogram() { return new MeterFilter() { @Override public DistributionStatisticConfig configure(Meter.Id id, DistributionStatisticConfig config) { if (id.getName().startsWith("http.server.requests")) { return DistributionStatisticConfig.builder() .percentiles(0.5, 0.95, 0.99, 0.999) .build() .merge(config); } return config; } }; } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After the histogram is available, the metrics become much richer and are easy to turn into alerts.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;http_server_requests_seconds{method="GET",outcome="SUCCESS",status="200",uri="/customers",quantile="0.5",} 0.00458752 http_server_requests_seconds{method="GET",outcome="SUCCESS",status="200",uri="/customers",quantile="0.95",} 0.011927552 http_server_requests_seconds{method="GET",outcome="SUCCESS",status="200",uri="/customers",quantile="0.99",} 0.04390912 http_server_requests_seconds{method="GET",outcome="SUCCESS",status="200",uri="/customers",quantile="0.999",} 0.04390912 http_server_requests_seconds_count{method="GET",outcome="SUCCESS",status="200",uri="/customers",} 51.0 http_server_requests_seconds_sum{method="GET",outcome="SUCCESS",status="200",uri="/customers",} 0.308183441&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you want to ignore HTTP metrics related to anything in the dev tools &lt;code&gt;/q&lt;/code&gt; endpoints, you can add the following to the configuration file.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt;quarkus: micrometer: binder: http-server: ignore-patterns: - '/q.*'&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;The amount of change that has come to the Quarkus ecosystem in the past year is really amazing. While the framework still aims to be lightweight and fast, its tooling and overall usability seem to be on an exponential curve of increasing developer productivity. If you are looking for a Java framework as part of an effort around application modernization, or are beginning your journey into &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;-native application development, then Quarkus should definitely be on your shortlist.&lt;/p&gt; &lt;p&gt;Once you're ready to dive in, you can &lt;a href="https://github.com/quarkus-ground-up/customer-api/tree/0.0.1"&gt;download the complete source code for this article&lt;/a&gt;.&lt;/p&gt; &lt;h3 id="where_to_learn_more-h2"&gt;Where to learn more&lt;/h3&gt; &lt;p&gt;Find additional resources:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Explore &lt;a href="https://developers.redhat.com/articles/2021/05/31/learn-quarkus-faster-quick-starts-developer-sandbox-red-hat-openshift"&gt;Quarkus quick starts&lt;/a&gt; in the &lt;a href="https://red.ht/dev-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;, which offers a free and ready-made environment for experimenting with containerized applications.&lt;/li&gt; &lt;li&gt;Try free &lt;a href="https://learn.openshift.com/developing-with-quarkus/"&gt;15-minute interactive learning scenarios&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Download the e-book &lt;a href="https://red.ht/quarkus-spring-devs"&gt;&lt;em&gt;Quarkus for Spring Developers&lt;/em&gt;&lt;/a&gt;, which provides a detailed look at Quarkus fundamentals and offers side-by-side examples of familiar Spring concepts, constructs, and conventions.&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/02/03/build-rest-api-ground-quarkus-20" title="Build a REST API from the ground up with Quarkus 2.0"&gt;Build a REST API from the ground up with Quarkus 2.0&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Stephen Nimmo</dc:creator><dc:date>2022-02-03T07:00:00Z</dc:date></entry><entry><title type="html">KIE Sandbox: top 7 key new features</title><link rel="alternate" href="https://blog.kie.org/2022/02/kie-sandbox-top-7-key-new-features.html" /><author><name>Eder Ignatowicz</name></author><id>https://blog.kie.org/2022/02/kie-sandbox-top-7-key-new-features.html</id><updated>2022-02-03T05:00:00Z</updated><content type="html">In the last months of 2021, the “.NEW environment” (bpmn.new, dmn.new) received a massive update, and now it’s named ! Dealing with complex models and collaborating with others has just become much easier. In this blog post, let’s go for a walkthrough of the top new features of KIE Sandbox. A FRESH “.NEW” HOME PAGE We launched a brand new home page. One of the cool additions to our home is the ‘Recent models widget’, allowing quick access to your recent models and also, now can import projects direct from GitHub, a long-waited feature! MULTI-FILE SUPPORT KIE Sandbox now supports multiple files! Now you can have a set of numerous DMN and BPMN models and reference those, i.e., include a DMN Model to reuse his datatypes. TEST YOUR DMN MODELS Since some releases ago, on every change of your DMN model, we will combine your DMN model and your form input and evaluate it on the DMN Engine. The significant part of this workflow is that it is really fast, looking almost instantaneous. Now on Kie Sandbox, you can test multiple inputs simultaneously with our table view. See in action: GITHUB INTEGRATION One of the critical highlights of Kie Sandbox is the integration with GitHub repositories. You can now import a repository, create a repository from an ephemeral set of files, and push/Pull directly to/from a GitHub repository. VS CODE INTEGRATION With a single click on our UI, you can go to VS Code Desktop and vscode.dev and keep working on our Kie Sandbox models there! TRY ON OPEN SHIFT Do you want to share your decisions with someone to let them give it a try on your models? Just click on ‘Try on OpenShift’ to deploy your Decisions on Developer Sandbox for OpenShift. DEPLOY YOUR OWN VERSION ON OPEN SHIFT Do you want to run KIE Sandbox in your cluster? The Kogito Tooling release 0.16.0 includes three container images to make it easy to deploy the KIE Sandbox to an OpenShift instance. Take a look at this for more details! HOW TO START TO USE IT? It’s super simple. Just access THANK YOU TO EVERYONE INVOLVED! I would like to thank everyone involved with this release, from the excellent KIE Tooling Engineers to the lifesavers QEs and the UX people that help us look awesome! The post appeared first on .</content><dc:creator>Eder Ignatowicz</dc:creator></entry><entry><title type="html">CUSTOMIZE PATH AND NAME OF GENERATED DMN/BPMN SVGs ON VS CODE</title><link rel="alternate" href="https://blog.kie.org/2022/02/customize-path-and-name-of-generated-dmn-bpmn-svgs-on-vs-code.html" /><author><name>Thiago Lugli</name></author><id>https://blog.kie.org/2022/02/customize-path-and-name-of-generated-dmn-bpmn-svgs-on-vs-code.html</id><updated>2022-02-02T14:39:15Z</updated><content type="html">To allow new automated workflows and more customizability when creating or editing .dmn/.bpmn files on VS Code, on Kogito Tooling 0.16.0, we introduced new extension settings that allow users to define where to save and what to call the generated SVG files. Check out the example below: AVAILABLE SETTINGS Both DMN e BPMN Editors now have this feature, so to start using it, all you have to do is configure the new available settings: * kogito.{dmn|bpmn}.svgFilenameTemplate * kogito.{dmn|bpmn}.svgFilePath For this, you will need to access the settings page on VS Code or edit your settings.json file directly: * On Windows/Linux: File &gt; Preferences &gt; Settings * On macOS: Code &gt; Preferences &gt; Settings Svg Filename Template: Define the name of the generated SVG file. Note: It must not contain a sub-path, as this should be defined in the SVG File Path setting. SVG File Path: This setting is used to define the path where the generated SVG file should be saved. Note: It must be a writable path anywhere on your file system. For both settings, some predefined variables can be used to compose their values, as they will be replaced with the corresponding values according to the table below. PREDEFINED VARIABLES Let’s say you have a workspace with the following structure opened on your VS Code instance and you are editing myDecision1.dmn on the DMN Editor extension: * myProject/ * decisions/ * myDecision1.dmn * myDecision2.dmn * src/ * … These are the variables available and their respective values for the case described above: TokenValue${workspaceFolder}/user/home/myProject${fileDirname}/user/home/myProject/decisions${fileExtname}.dmn${fileBasename}myDecision1.dmn${fileBasenameNoExtension}myDecision1 These predefined variables were based on VS Code’s , except they are only available for tasks.json and launch.json configuration files. (Support for the settings.json configuration file is being tracked on the following issues: and ) EXAMPLE OF USAGE Using the same workspace structure detailed above we might want to save generated SVG files in the myProject/assets/decisions/.dmn/ directory with the following name: "myDecision1.svg". For that the settings should have the following values: kogito.dmn.svgFilenameTemplate${fileBasenameNoExtension}.svgkogito.dmn.svgFilePath${workspaceFolder}/assets/decisions/${fileExtname}/ The same goes for BPMN files. Feel free to leave your feedback and questions in the comment section! The post appeared first on .</content><dc:creator>Thiago Lugli</dc:creator></entry><entry><title>Protect secrets in Git with the clean/smudge filter</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/02/02/protect-secrets-git-cleansmudge-filter" /><author><name>Tomer Figenblat</name></author><id>284b2031-ea69-425b-a84c-8e6c3506565d</id><updated>2022-02-02T07:00:00Z</updated><published>2022-02-02T07:00:00Z</published><summary type="html">&lt;p&gt;When working on public &lt;a href="https://developers.redhat.com/topics/gitops"&gt;Git&lt;/a&gt; repositories, you need to pay close attention so that you don't accidentally push secret information such as tokens, private server addresses, personal email addresses, and the like. One of the tools that can help you is Git's clean/smudge filter.&lt;/p&gt; &lt;h2&gt;Clean and smudge your Git repository &lt;/h2&gt; &lt;p&gt;The clean/smudge filter is quite simple to use. Create a filter driver with two commands—&lt;code&gt;clean&lt;/code&gt; and &lt;code&gt;smudge&lt;/code&gt;—and then apply the filter per record in the &lt;code&gt;.gitattributes&lt;/code&gt; file.&lt;/p&gt; &lt;p&gt;This makes the filter stand between the working directory and the staging area for the specific &lt;code&gt;.gitattributes&lt;/code&gt; record. When adding content from the working directory to the staging area with &lt;code&gt;git add&lt;/code&gt;, files that match the &lt;code&gt;.gitattributes&lt;/code&gt; record will go through the &lt;code&gt;clean&lt;/code&gt; filter. When pulling content back into the working directory with &lt;code&gt;git pull&lt;/code&gt;, those same files will go through the &lt;code&gt;smudge&lt;/code&gt; filter.&lt;/p&gt; &lt;h2&gt;Create the filter driver&lt;/h2&gt; &lt;p&gt;Let's create a filter driver named &lt;code&gt;cleanPass&lt;/code&gt; that uses &lt;code&gt;sed&lt;/code&gt; expressions to replace the value &lt;code&gt;secretpassword&lt;/code&gt; with the value &lt;code&gt;hiddenpassword&lt;/code&gt; and vice versa:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;git config filter.cleanPass.clean "sed -e 's/secretpassword/hiddenpassword/g'" git config filter.cleanPass.smudge "sed -e 's/hiddenpassword/secretpassword/g'" &lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Tip&lt;/strong&gt;: Add &lt;code&gt;--global&lt;/code&gt; to the &lt;code&gt;config&lt;/code&gt; command to make the filter driver available globally to all repositories at &lt;code&gt;~/.gitconfig&lt;/code&gt; instead of locally at &lt;code&gt;.git/config&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;As an example, let's apply the &lt;code&gt;cleanPass&lt;/code&gt; filter driver to the JSON file type. In the repository's &lt;code&gt;.gitattributes&lt;/code&gt;, look for the JSON file type record and modify it. The result should look something like this, depending on the original configuration:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;*.json text eol=lf filter=cleanPass &lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Tip&lt;/strong&gt;: To avoid pushing, this can be done in &lt;code&gt;.git/info/attributes&lt;/code&gt;, which takes precedence over &lt;code&gt;.gitattributes&lt;/code&gt;. Just be sure not to mess up any of the original record configurations like &lt;code&gt;eol&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;From now on, every new or modified JSON file in the repository will go through the &lt;code&gt;cleanPass&lt;/code&gt; filter. Figure 1 provides an animated look at what happens next.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/git-filter.gif"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/git-filter.gif" width="988" height="503" alt="git-clean-smudge-filter" loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;span class="field field--name-field-creator field--type-string field--label-inline field__items"&gt; &lt;span class="field__label"&gt;Creator&lt;/span&gt; &lt;span class="rhd-media-creator field__item"&gt; Tomer Figenblat &lt;/span&gt; &lt;/span&gt; &lt;span class="field field--name-field-license field--type-entity-reference field--label-inline field__items"&gt; &lt;span class="field__label"&gt;License&lt;/span&gt; &lt;span class="rhd-media-licence field__item"&gt; under &lt;a href="https://opensource.org/licenses/MIT" title="MIT License"&gt;MIT License&lt;/a&gt;. &lt;/span&gt; &lt;/span&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1. The clean filter in action.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;One filter to rule them all&lt;/h2&gt; &lt;p&gt;That was cool, but using this filter can get quite cumbersome at times. For instance, on a recent task, I worked with multiple repositories using multiple (but identical) services from my team's &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; lab. To add to the complexity, I usually work from two different computers—my desktop at my office and my laptop at home.&lt;/p&gt; &lt;p&gt;That all means that I needed to create multiple filters (or one complex one) and apply them to multiple file types, for multiple repositories, on multiple computers. That gave me a headache, especially when the fact hit me that this workflow will probably reoccur in future tasks.&lt;/p&gt; &lt;p&gt;Eventually, I decided to create a very simple script that handles multiple value replacements in both directions, save it on each of my computers, and configure it as a global filter.&lt;/p&gt; &lt;h3&gt;Configure a global filter&lt;/h3&gt; &lt;p&gt;Adding values to be replaced in this script is super easy, and attaching it to &lt;code&gt;.gitattributes&lt;/code&gt; records, as seen above, is quite simple as well.&lt;/p&gt; &lt;p&gt;First, create a script in a path available to all repositories. For instance, you could put it in your home directory at &lt;code&gt;~/scripts/git-smudge-clean-filter.sh&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;#!/bin/bash declare -A mapArr mapArr["my-work-private-server.mywork.com"]="&lt;reducted-work-server&gt;" mapArr["my-personal-private-server.myowndomain.org"]="&lt;reducted-personal-server&gt;" mapArr["A*&amp;#QAADDA(77##F"]="super-secret-token" mapArr["oops@mypersonal.email"]="support@correct.email" sedcmd="sed" if [[ "$1" == "clean" ]]; then for key in ${!mapArr[@]}; do sedcmd+=" -e \"s/${key}/${mapArr[${key}]}/g\"" done elif [[ "$1" == "smudge" ]]; then for key in ${!mapArr[@]}; do sedcmd+=" -e \"s/${mapArr[${key}]}/${key}/g\"" done else echo "use smudge/clean as the first argument" exit 1 fi eval $sedcmd&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Add as many pairs as you like to &lt;code&gt;mapArr&lt;/code&gt;.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; Per the nature of Git's filter driver, you can't use the same key or value more than once.&lt;/p&gt; &lt;p&gt;Next, create the filter driver globally (note the script argument):&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;git config --global filter.reductScript.smudge "~/scripts/git-smudge-clean-filter.sh smudge" git config --global filter.reductScript.clean "~/scripts/git-smudge-clean-filter.sh clean"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now, for every &lt;code&gt;.gitattributes&lt;/code&gt; record to which you apply the &lt;code&gt;filter=reductScript&lt;/code&gt; configuration in any repository, every file matching this record will go through the script, and every value specified in &lt;code&gt;mapArr&lt;/code&gt; will be reducted (or returned).&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;That's all there is to it. I hope you find this useful. We'll cover other Git tips in future articles.&lt;/p&gt; &lt;p&gt;In the meantime, check out the following Git resources:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://gist.github.com/TomerFi/0911f573ea0474b9ab74bcfcef0f2a49"&gt;My GitHub gist&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://git-scm.com/docs/gitattributes"&gt;gitattributes documentation&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://git-scm.com/docs/git-config"&gt;git-config documentation&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://git-scm.com/docs/git-config#Documentation/git-config.txt-filterltdrivergtclean"&gt;Git filter.&lt;driver&gt;.clean command&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://git-scm.com/docs/git-config#Documentation/git-config.txt-filterltdrivergtsmudge"&gt;Git filter.&lt;driver&gt;.smudge command&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/cheat-sheets/git/old"&gt;Git Cheat Sheet&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/02/02/protect-secrets-git-cleansmudge-filter" title="Protect secrets in Git with the clean/smudge filter"&gt;Protect secrets in Git with the clean/smudge filter&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Tomer Figenblat</dc:creator><dc:date>2022-02-02T07:00:00Z</dc:date></entry><entry><title type="html">Intelligent data as a service (iDaaS) - Example data insights</title><link rel="alternate" href="http://www.schabell.org/2022/01/idaas-example-data-insights.html" /><author><name>Eric D. Schabell</name></author><id>http://www.schabell.org/2022/01/idaas-example-data-insights.html</id><updated>2022-02-02T06:00:00Z</updated><content type="html">Part 4 - Example HL7 &amp;amp; FHIR integration In our  from this series we talked about the example iDaaS data architecture specifically integration for HL7 and FHIR healthcare standards. The process was laid out how we approached the use case and how portfolio solutions are the base for researching a generic architecture. It continued by laying out the process of how we approached the use case by researching successful customer portfolio solutions as the basis for a generic architecture. Having completed our exploration of an HL7 and FHIR integration architecture, we'll walk you through a more specific example. This scenario is showing a proven and scalable architecture for integrating data insights into your healthcare solutions. ARCHITECTURE REVIEW As mentioned before, the architectural details covered here are based on real solutions using open source technologies. The example scenario presented here is a generic common architecture that was uncovered while researching those solutions. It's our intent to provide guidance and not deep technical details. This section covers the visual representations as presented, but it's expected that they'll be evolving based on future research. There are many ways to represent each element in this architecture, but we've chosen a format that we hope makes it easy to absorb. Feel free to post comments at the bottom of this post, or  with your feedback. Now let's take a look at the details in this architecture and outline the solution. IDAAS INSIGHTS In the previous article, details were given for a specific integration architecture for iDaaS that focused on HL7 and FHIR healthcare standards. In this example the focus moves to gathering knowledge about the iDaaS solutions and ensure that compliancy and conformance is met. This diagram also simplifies the iDaaS Connect collection by reducing it to a single FHIR connect microservices element working with message transformation microservices element. With that in mind, let's starting on the left side of the diagram where the entry point for actors in this architecture can be found. They're representing the edge devices as well as the users submitting requests and receiving responses.  These users are representing patients, vendors, suppliers, and clinical personnel. The administrators is representing the clinical personnel tasked with managing the various healthcare processes. All requests enter through the API management element, used to secure and authenticate access to internal services and applications. The first collection of elements encountered is labeled as iDaaS Connect, where we find the single integration service for FHIR communication channels as discussed in the previously.  After all of the standards integration and eventual message transformation activities, iDaaS Connect services register events (and receive event notification from) the iDaaS connect events. This is a central hub that ensures all events are registered, managed, and notifications are sent to the appropriate elements in the iDaaS architecture.  Events will often trigger elements of the iDaaS DREAM collection to take action, through the iDaaS event builder which captures business automation activities and the iDaaS intelligent data router. This data router is capable of managing where specific data needs to be sent, both inbound to sources and outbound to application or service destinations. It's assisted by the iDaaS connect data distribution element which ensures integration with all manner of data sources which might be in local or remote locations such as a public cloud. There are all manner of possible, and optional, external services that a healthcare organisation might integrate from third-party or externally hosted services such as reporting services, security, analytics, monitoring &amp;amp; logging, external API management, big data, and other partner data services. Finally, we reach the focus of this example. This iDaaS architecture provides both conformance and insights into the knowledge being managed by the offered solutions. The iDaaS knowledge insight element manages analytics and insights into the data available across the live platform. This can be setup to provide near-realtime gathering and reporting as organisational need require. An essential requirement for any healthcare organisation is to maintain compliancy to national laws, data privacy, and other transitional requirements. The iDaaS knowledge conformance element is a set of applications and tools that allow for any organisation to automate compliancy and regulation adherence using rule systems customised to their own local needs. This is a very flexible element that can be designed to ensure that compliancy audit requirements are constantly met. Access is shown for the compliancy officer or similar user for maintaining, monitoring, and reporting on compliancy data. Next up, a look at the architectural solution with a focus on the data view. IDAAS INSIGHTS (DATA) Data insights through the iDaaS insights architecture provides a different look at the architecture and gives us insights into how our healthcare solutions are performing. It should be seen as the architecture is intended, as a guide and not a definitive must-do-it-this-way statement on how the data is being routed through as actors are engaging with the systems, applications, and services in this architecture. Note that many of the data flows only one direction while it's fairly obvious it's going to flow both ways. We've chosen to note that in the flows that do not disrupt the clarity of the architecture diagram, and chose not to indicate where the intent is to show processing and flows for clarity from actors to systems on the backend. It's left to the reader to explore these data diagrams and feel free to send comments our way. WHAT'S NEXT This was just a short overview of an example data insights architecture for the intelligent data as a service architecture.  An overview of this series on intelligent data as a service architecture: 1. 2. 3. 4. 5. Catch up on any articles you missed by following one of the links above. This completes the series and we hope you enjoyed this tour of the intelligent data as a service architecture.</content><dc:creator>Eric D. Schabell</dc:creator></entry></feed>
